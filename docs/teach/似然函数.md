# 量子层析重构中的似然函数详解

> 从直觉 → 公式 → Python实现 → 对比现有χ²实现，彻底理解似然函数、NLL与MLE的关系

---

## 🧩 一、核心概念：Likelihood、Log-Likelihood、NLL

| 名称 | 英文 | 数学意义 | 优化中用法 |
|------|------|----------|------------|
| **似然函数** | Likelihood | 衡量"参数下数据出现的概率" | 我们想**最大化它** |
| **对数似然** | Log-Likelihood | 取对数，方便求导 | 同样要**最大化它** |
| **负对数似然** | Negative Log-Likelihood (NLL) | 加一个负号 | 转为**最小化问题** |

### 数学关系

$$\text{NLL}(\theta) = -\log L(\theta)$$

**MLE（最大似然估计）**：
$$\hat{\theta}_{MLE} = \arg\max_\theta L(\theta) = \arg\min_\theta [-\log L(\theta)] = \arg\min_\theta \text{NLL}(\theta)$$

➡️ **NLL 就是似然函数的负对数形式**。用哪个形式取决于是"最大化"还是"最小化"问题，内容本质相同。

---

## 🧠 二、量子层析中的似然函数

### 理论概率

在量子层析实验中，每个测量结果的理论概率是：
$$p_i(\rho) = \mathrm{Tr}(P_i \rho)$$

其中：
- $P_i$：测量投影算符
- $\rho$：待估计的密度矩阵

### 观测结果

- 每个投影 $P_i$ 下，观测到 $n_i$ 次
- 总测量次数 $N = \sum_i n_i$

---

## 🧮 三、精确的似然函数

如果测量是**多项式分布**（Multinomial），则：

$$L(\rho) = \Pr(\{n_i\}|\rho) = \frac{N!}{\prod_i n_i!} \prod_i [p_i(\rho)]^{n_i}$$

> **解释**：每个事件的概率是 $p_i(\rho)$，独立重复 $N$ 次测量后得到 $n_i$ 次。

---

## 📉 四、对数似然（Log-Likelihood）

取对数（方便求导）：

$$\log L(\rho) = \text{常数} + \sum_i n_i \log p_i(\rho)$$

常数项（$\log N! - \sum_i \log n_i!$）与 $\rho$ 无关，可忽略。

---

## 🔻 五、负对数似然（NLL）形式

**最常用的优化目标**：

$$\text{NLL}(\rho) = -\sum_i n_i \log p_i(\rho) = -\sum_i n_i \log \mathrm{Tr}(P_i \rho)$$

👉 这就是**标准 MLE 里使用的 NLL 目标函数**。我们希望最小化它。

---

## ⚙️ 六、对应的梯度

对 $\rho$ 的梯度（用于数值优化）：
$$\nabla_\rho \text{NLL} = -\sum_i \frac{n_i}{\mathrm{Tr}(P_i \rho)} P_i$$

梯度方向指示"如果增加某些投影的概率能降低NLL"。

---

## 🧩 七、与当前χ²近似目标对比

你目前在 `mle.py` 里的目标函数，**不是 NLL**，而是类似下面这种：

**Chi²形式**：
$$\chi^2(\rho) = \sum_i \frac{(n_i/N - p_i(\rho))^2}{p_i(\rho)}$$

**简化版本**：
$$\text{Loss}(\rho) = \sum_i (n_i/N - p_i(\rho))^2$$

### 对比分析

| 对比项 | χ² 近似 | 真实 NLL |
|--------|---------|----------|
| **来历** | 来自二阶近似于 log-likelihood 的泰勒展开 | 精确的对数似然形式 |
| **凸性** | 一般非凸 | 对 ρ 是凸函数 |
| **统计意义** | 近似正确，仅在大样本/高信噪时合理 | 完全正确，收敛到真态 |
| **优化难度** | 简单，梯度易算 | 稍复杂但稳定 |
| **主流程度** | 快速原型/早期实验 | ✅ 论文级/实验室标准 |
| **是否物理一致** | 可能给出非物理态 | 与 Born rule 完全对齐 |

---

## 💡 八、为什么大家偏向 NLL

### 1. 凸优化问题
- 对 $\rho$ 而言，$-\log(\mathrm{Tr}(P_i \rho))$ 是凸函数
- 因此整个 NLL 是凸的（**唯一全局最优**）

### 2. 统计无偏 & 渐近效率
- MLE（基于 NLL）有最优的渐近方差
- χ² 只是线性近似，偏差更大

### 3. 物理一致性
- NLL 直接反映测量概率与实验频率
- 适用于任意噪声、零计数、泊松计数模型

---

## 🧾 九、NLL 在 Python 实现中的典型写法

### 目标函数
```python
def negative_log_likelihood(rho, projectors, counts):
    eps = 1e-12
    p = np.array([np.trace(P @ rho).real for P in projectors])
    p = np.clip(p, eps, None)
    return -np.sum(counts * np.log(p))
```

### 梯度
```python
def nll_grad(rho, projectors, counts):
    eps = 1e-12
    p = np.array([np.trace(P @ rho).real for P in projectors])
    coeff = -(counts / np.clip(p, eps, None))
    G = sum(c * P for c, P in zip(coeff, projectors))
    return (G + G.conj().T) / 2  # 保证Hermitian
```

---

## 🔍 十、泊松噪声模型（另一种常见版本）

假设每个测量结果是独立泊松分布：
$$L(\rho) = \prod_i \frac{[\lambda_i(\rho)]^{n_i} e^{-\lambda_i(\rho)}}{n_i!}$$

则：
$$\text{NLL} = \sum_i [\lambda_i(\rho) - n_i \log \lambda_i(\rho)]$$

泊松与多项式模型在实验上都常见；区别仅在是否总和固定。

---

## ✅ 十一、总结对比

| 项目 | χ² 拟合（你现在） | 真实 NLL（推荐） |
|------|------------------|------------------|
| **目标函数** | $\sum \frac{(p_{obs} - p_{exp})^2}{p_{exp}}$ | $-\sum n_i \log p_i(\rho)$ |
| **凸性** | 否 | ✅ 是 |
| **统计一致性** | 近似 | ✅ 真正MLE |
| **全局最优保证** | ❌ 无 | ✅ 有 |
| **梯度** | 线性差异 | 有解析表达式 |
| **推荐场景** | 快速原型、调试 | 论文级、实验室分析 |

---

## 🧭 十二、改进建议

### 短期（不改结构）
- 在现有 MLE 类中加一个 `mode='chi2' | 'nll'` 参数
- 保留两种目标函数可切换
- 这样不破坏旧代码

### 长期（论文级别）
- 全面迁移到 NLL
- 实现 RρR 或 PGD 算法
- 验证 χ² 与 NLL 的结果差异（例如用保真度）

---

## 🎯 结论

> 你现在的似然函数是**χ² 型近似似然**，而**NLL（Negative Log-Likelihood）**是正宗的、严格凸的、理论上正确的似然函数形式。它才是 MLE 的真正目标函数：

$$\boxed{\text{NLL}(\rho) = -\sum_i n_i \log \mathrm{Tr}(P_i \rho)}$$
