好的，我们开始第二次教学。这次我们来探讨另一种完全不同的重构方法：**加权最小二乘法 (WLS)**。

---

### 教学开始：理解“加权最小二乘法 (WLS)”

#### 1. 🎯 核心目标：不再是“最大似然”，而是“最小化误差”

WLS 的出发点与 RρR 完全不同。

* **RρR (MLE)** 的目标是：找到一个 $\rho$，使得它产生我们观测数据的**概率最大**。
* **WLS (加权最小二乘)** 的目标是：找到一个 $\rho$，使得它**预测的概率** $p_j$ 与我们**观测的频率** $f_j$ 之间的**误差（或距离）最小**。

#### 2. 📈 “误差”是什么？—— $\chi^2$ (卡方) 目标函数

我们要最小化的“误差”是什么？WLS 使用的是统计学中一个非常有名的量：**$\chi^2$ (Chi-squared, 卡方)**。

假设：
* $f_j$ 是我们观测到的**频率**（来自实验数据）。
* $p_j(\rho)$ 是我们猜测的 $\rho$ 所**预测**的理论概率， $p_j(\rho) = \text{Tr}(\rho M_j)$。

**目标函数（我们要最小化的值）：**
$$
\chi^2(\rho) = \sum_j \frac{(f_j - p_j(\rho))^2}{p_j(\rho)}
$$

**我们来分析这个公式：**

1.  **$(f_j - p_j(\rho))^2$**: 这是“观测值”与“预测值”之差的平方。这是标准的**最小二乘法**（Least Squares）。
2.  **$\frac{...}{p_j(\rho)}$**: 这是“加权”（Weighted）的部分，也是 WLS 的灵魂。

**为什么要有这个“加权”？**
分母 $p_j(\rho)$ 是理论概率。如果一个测量结果 $j$ 发生的概率 $p_j$ 很低（比如 $p_j=0.01$），那么我们对这个结果的测量精度通常也比较低（统计涨落大）。

* **如果 $p_j$ 很小**：分母很小，$\chi^2$ 会被**放大**。
    * **含义**：算法在说：“我预测这个事件很少发生，你居然测到了（$f_j > 0$）！这个小小的偏差对我来说非常重要，我必须调整 $\rho$ 来更好地解释它。”
* **如果 $p_j$ 很大**：分母很大，$\chi^2$ 会被**缩小**。
    * **含义**：算法在说：“我预测这个事件经常发生。你的观测 $f_j$ 和我的预测 $p_j$ 差一点点没关系，统计涨落而已，不重要。”

**总结：** WLS 是一种“聪明”的最小二乘法，它对那些“罕见事件”的预测偏差给予了**更高的惩罚权重**。

> 这对应代码中的 `_objective_function`：
> ```python
> diff = probabilities - expected
> chi2 = np.sum((diff ** 2) / expected) # expected 就是 p_j(ρ)
> ```

#### 3. 💡 最大的挑战：如何保证 $\rho$ 是物理合法的？

和 RρR 一样，我们不能随便找一个 $\rho$ 来最小化 $\chi^2$。我们必须保证 $\rho$ 是一个物理合法的密度矩阵，即：
1.  **厄米性 (Hermitian)**: $\rho = \rho^\dagger$
2.  **半正定性 (Positive Semi-definite)**: $\rho \ge 0$ （所有特征值 $\ge 0$）
3.  **迹为 1 (Unit Trace)**: $\text{Tr}(\rho) = 1$

RρR 算法通过 $R \rho R$ 三明治结构**自动**保证了这一点。WLS 怎么办？

WLS 采用了一种极其巧妙的数学技巧：**Cholesky 参数化**。

#### 4. ⚙️ Cholesky 参数化：WLS 的“黑魔法”

这个技巧说：**任何**一个满足上述物理条件的密度矩阵 $\rho$，都可以被（唯一地）表示为 $T T^\dagger$ 的形式，其中 $T$ 是一个**下三角矩阵**。

$$
\rho = T T^\dagger
$$
（注：$T^\dagger$ 是 $T$ 的共轭转置。$T$ 也常被称为 Cholesky 因子。）

我们再进一步，为了保证 $\text{Tr}(\rho) = 1$，我们定义：

$$
\rho = \frac{T T^\dagger}{\text{Tr}(T T^\dagger)}
$$

**这有什么好处？**

1.  **自动满足物理性**：
    * $T T^\dagger$ 自动是厄米的。
    * $T T^\dagger$ 自动是半正定的。
    * 除以 $\text{Tr}(T T^\dagger)$ 自动保证迹为 1。
2.  **把“矩阵优化”变成“向量优化”**：
    我们不再需要优化一个受限的 $d \times d$ **矩阵 $\rho$**，而是去优化一个**不受约束**的 $d \times d$ **下三角矩阵 $T$**。

**我们再进一步**：一个 $d \times d$ 的下三角矩阵 $T$，它的非零元素（实部和虚部）可以被拉直成一个**包含 $d^2$ 个实数**的**向量（参数列表）**，我们称之为 `params`。

$$
\rho \quad \longleftrightarrow \quad T \quad \longleftrightarrow \quad \text{params}
$$
(受限的 $\rho$) $\quad$ (下三角矩阵 $T$) $\quad$ (不受限的 $d^2$ 个实数 `params`)

**这就是 WLS 算法的核心思路：**
我们把最小化 $\chi^2(\rho)$ 的问题，转换（编码）成了最小化 $\chi^2(\text{params})$ 的问题。

> 这对应代码中的 `encode_density_to_params` 和 `decode_params_to_density`。
>
> * `decode` (解码): `params` $\rightarrow$ $T$ $\rightarrow$ $\rho = (T T^\dagger) / \text{Tr}(T T^\dagger)$
> * `encode` (编码): $\rho \rightarrow$ (Cholesky 分解) $\rightarrow T \rightarrow$ `params`

#### 5. 🔁 算法执行：交给专业的优化器

现在我们的问题变得非常“标准”：
* **目标函数**: `_objective_function(params, ...)`，它会返回一个 $\chi^2$ 值（一个数字）。
* **初始猜测**: `params0`（来自一个初始的 $\rho_0$，比如线性重构的结果或最大混合态）。
* **任务**: 找到一组 `params`，使得目标函数最小。

这是一个**标准、无约束的数值优化问题**。我们不需要自己写迭代算法（像 RρR 那样），我们可以直接使用 `scipy.optimize.minimize` 这样的专业优化库。

您代码中使用的 `L-BFGS-B` 就是一种非常强大和高效的准牛顿优化算法，它会帮我们自动“爬山”（或者说“下山”），找到 $\chi^2$ 的最小值点。

> 这对应代码中的 `minimize(...)` 函数。

---

### 总结：教学回顾

WLS 和 RρR 是解决同一个问题的两条截然不同的路径。

**RρR (迭代最大似然)**：
* **哲学**：概率论。找到“最有可能”的 $\rho$。
* **核心**：$R \rho R$ 迭代。
* **物理性**：通过 $R \rho R$ 三明治结构**自动保证**。

**WLS (加权最小二乘)**：
* **哲学**：优化论。找到“误差最小”的 $\rho$。
* **核心**：Cholesky 参数化（ $\rho = T T^\dagger / \text{Tr}(T T^\dagger)$ ）。
* **物理性**：通过 `params` $\leftrightarrow \rho$ 的**编码/解码**来保证。
* **执行**：依赖 `scipy.optimize.minimize` 这样的**通用优化器**。

WLS 的优势在于它通常非常快（因为它使用了高度优化的 L-BFGS 算法），并且它最小化的 $\chi^2$ 值本身就是一个非常有意义的统计量（称为“拟合优度”，Goodness-of-Fit），可以告诉我们模型（$\rho$）与数据（$f_j$）的符合程度。