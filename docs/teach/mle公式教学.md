# MLEReconstructor ç¨‹åºä¸­çš„æ•°å­¦ä¸ç‰©ç†å…¬å¼è®²è§£

> ä»¥ä¸‹å†…å®¹æŒ‰ **æ¦‚å¿µ â†’ å…¬å¼ â†’ æ¨å¯¼ â†’ ç‰©ç†æ„ä¹‰ â†’ ä»£ç å¯¹åº”ä½ç½®** é¡ºåºè®²è§£ï¼Œæ·±å…¥ç†è§£æœ€å¤§ä¼¼ç„¶ä¼°è®¡é‡å­å±‚æçš„æ•°å­¦åŸºç¡€ã€‚

---

## ğŸ”¬ ä¸€ã€æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„ç»Ÿè®¡å­¦åŸºç¡€

### 1. ä»€ä¹ˆæ˜¯æœ€å¤§ä¼¼ç„¶ä¼°è®¡ (MLE)ï¼Ÿ

#### åŸºæœ¬æ€æƒ³

ç»™å®šè§‚æµ‹æ•°æ® $\{x_1, x_2, \ldots, x_N\}$ï¼Œå¯»æ‰¾å‚æ•° $\theta$ ä½¿å¾—è§‚æµ‹åˆ°è¿™äº›æ•°æ®çš„æ¦‚ç‡æœ€å¤§ï¼š

$$
\hat{\theta} = \arg\max_{\theta} P(x_1, x_2, \ldots, x_N | \theta)
$$

**ç‰©ç†æ„ä¹‰**ï¼š
- "ä»€ä¹ˆæ ·çš„é‡å­æ€æœ€å¯èƒ½äº§ç”Ÿæˆ‘ä»¬è§‚æµ‹åˆ°çš„æµ‹é‡ç»“æœï¼Ÿ"
- ç¬¦åˆ"å¥¥å¡å§†å‰ƒåˆ€"åŸåˆ™ï¼šé€‰æ‹©æœ€ç®€å•çš„è§£é‡Š

---

### 2. é‡å­æ€å±‚æçš„ä¼¼ç„¶å‡½æ•°

#### å¤šé¡¹å¼åˆ†å¸ƒ

åœ¨é‡å­æ€å±‚æä¸­ï¼Œæµ‹é‡ç»“æœæœä»**å¤šé¡¹å¼åˆ†å¸ƒ**ï¼š

$$
P(n_1, n_2, \ldots, n_{n^2} | \rho) = \frac{N!}{n_1! n_2! \cdots n_{n^2}!} \prod_{i=1}^{n^2} p_i^{n_i}
$$

å…¶ä¸­ï¼š
- $N = \sum_i n_i$ï¼šæ€»æµ‹é‡æ¬¡æ•°
- $n_i$ï¼šæµ‹åˆ°ç»“æœ $i$ çš„æ¬¡æ•°
- $p_i = \mathrm{Tr}(E_i \rho)$ï¼šç†è®ºæ¦‚ç‡

---

#### ä¼¼ç„¶å‡½æ•°

$$
\mathcal{L}(\rho) = \prod_{i=1}^{n^2} p_i^{n_i} = \prod_{i=1}^{n^2} [\mathrm{Tr}(E_i \rho)]^{n_i}
$$

#### å¯¹æ•°ä¼¼ç„¶

ä¸ºäº†æ•°å€¼ç¨³å®šå’Œè®¡ç®—æ–¹ä¾¿ï¼Œå–å¯¹æ•°ï¼š

$$
\log \mathcal{L}(\rho) = \sum_{i=1}^{n^2} n_i \log[\mathrm{Tr}(E_i \rho)]
$$

**æœ€å¤§ä¼¼ç„¶ä¼°è®¡**ï¼š

$$
\hat{\rho} = \arg\max_{\rho} \sum_{i=1}^{n^2} n_i \log[\mathrm{Tr}(E_i \rho)]
$$

ç­‰ä»·äºæœ€å°åŒ–è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼š

$$
\hat{\rho} = \arg\min_{\rho} -\sum_{i=1}^{n^2} n_i \log[\mathrm{Tr}(E_i \rho)]
$$

**ç‰©ç†çº¦æŸ**ï¼š

$$
\begin{cases}
\rho = \rho^\dagger & \text{(å„ç±³æ€§)} \\
\rho \succeq 0 & \text{(æ­£å®šæ€§)} \\
\mathrm{Tr}(\rho) = 1 & \text{(å½’ä¸€æ€§)}
\end{cases}
$$

---

## ğŸ§® äºŒã€ä»ä¼¼ç„¶å‡½æ•°åˆ° ChiÂ² ç›®æ ‡å‡½æ•°

### 1. æ¦‚ç‡çš„å½’ä¸€åŒ–è¡¨ç¤º

åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬è§‚æµ‹åˆ°çš„æ˜¯**å½’ä¸€åŒ–æ¦‚ç‡**ï¼š

$$
\hat{p}_i = \frac{n_i}{N}
$$

å…¶ä¸­ $N = \sum_i n_i$ æ˜¯æ€»æµ‹é‡æ¬¡æ•°ã€‚

---

### 2. æ³Šæ¾è¿‘ä¼¼ä¸ ChiÂ² ç»Ÿè®¡é‡

#### å¤§æ ·æœ¬æé™

å½“æµ‹é‡æ¬¡æ•° $N$ å¾ˆå¤§æ—¶ï¼Œå¤šé¡¹å¼åˆ†å¸ƒå¯ä»¥ç”¨**ç‹¬ç«‹æ³Šæ¾åˆ†å¸ƒ**è¿‘ä¼¼ï¼š

$$
n_i \sim \text{Poisson}(N p_i)
$$

**æ³Šæ¾åˆ†å¸ƒçš„å¯¹æ•°ä¼¼ç„¶**ï¼š

$$
\log P(n_i | p_i) = n_i \log(N p_i) - N p_i - \log(n_i!)
$$

æ€»å¯¹æ•°ä¼¼ç„¶ï¼š

$$
\log \mathcal{L} = \sum_i [n_i \log(N p_i) - N p_i] + \text{const}
$$

---

#### æ³°å‹’å±•å¼€ä¸ ChiÂ² è¿‘ä¼¼

å¯¹äº $p_i$ æ¥è¿‘ $\hat{p}_i = n_i / N$ çš„æƒ…å†µï¼Œå¯¹æ•°ä¼¼ç„¶å¯ä»¥æ³°å‹’å±•å¼€ï¼š

$$
\log \mathcal{L} \approx \text{const} - \frac{N}{2} \sum_i \frac{(\hat{p}_i - p_i)^2}{p_i}
$$

**æ¨å¯¼**ï¼ˆäºŒé˜¶æ³°å‹’å±•å¼€ï¼‰ï¼š

è®¾ $f(p_i) = n_i \log p_i - N p_i$ï¼Œåœ¨ $p_i^* = \hat{p}_i$ å¤„å±•å¼€ï¼š

$$
f(p_i) \approx f(p_i^*) + f'(p_i^*)(p_i - p_i^*) + \frac{1}{2}f''(p_i^*)(p_i - p_i^*)^2
$$

å…¶ä¸­ï¼š
- $f'(p_i) = \frac{n_i}{p_i} - N$
- $f''(p_i) = -\frac{n_i}{p_i^2}$

åœ¨ $p_i^* = \hat{p}_i = n_i/N$ å¤„ï¼š
- $f'(p_i^*) = 0$ï¼ˆä¸€é˜¶é¡¹æ¶ˆå¤±ï¼‰
- $f''(p_i^*) = -\frac{N^2}{\hat{n}_i}$

å› æ­¤ï¼š

$$
f(p_i) \approx f(p_i^*) - \frac{N^2}{2 n_i}(p_i - \hat{p}_i)^2
$$

æ±‚å’Œå¾—ï¼š

$$
\log \mathcal{L} \approx \text{const} - \frac{N}{2} \sum_i \frac{(\hat{p}_i - p_i)^2}{\hat{p}_i}
$$

---

#### ChiÂ² ç»Ÿè®¡é‡

æœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶ç­‰ä»·äºæœ€å°åŒ–**ChiÂ² ç»Ÿè®¡é‡**ï¼š

$$
\chi^2 = \sum_{i=1}^{n^2} \frac{(\hat{p}_i - p_i)^2}{p_i}
$$

**æ³¨æ„**ï¼šåˆ†æ¯ç”¨ $p_i$ï¼ˆç†è®ºæ¦‚ç‡ï¼‰è€Œé $\hat{p}_i$ï¼ˆè§‚æµ‹æ¦‚ç‡ï¼‰ï¼Œè¿™åœ¨å®è·µä¸­æ›´ç¨³å®šã€‚

**ç‰©ç†æ„ä¹‰**ï¼š
- åŠ æƒæœ€å°äºŒä¹˜ï¼šè¯¯å·®æŒ‰æœŸæœ›æ¦‚ç‡åŠ æƒ
- $p_i$ å°æ—¶ï¼Œç»™äºˆè¾ƒå°æƒé‡ï¼ˆé¿å…æ”¾å¤§å°æ¦‚ç‡äº‹ä»¶çš„å™ªå£°ï¼‰
- ç¬¦åˆæ³Šæ¾ç»Ÿè®¡çš„æ–¹å·®æ€§è´¨ï¼š$\sigma_i^2 = p_i$

---

### 3. æœ€ç»ˆä¼˜åŒ–é—®é¢˜

$$
\hat{\rho} = \arg\min_{\rho} \chi^2(\rho) = \arg\min_{\rho} \sum_{i=1}^{n^2} \frac{(\hat{p}_i - \mathrm{Tr}(E_i \rho))^2}{\mathrm{Tr}(E_i \rho)}
$$

**çº¦æŸæ¡ä»¶**ï¼š

$$
\begin{cases}
\rho = \rho^\dagger \\
\rho \succeq 0 \\
\mathrm{Tr}(\rho) = 1
\end{cases}
$$

**ä»£ç å¯¹åº”**ï¼š
```python
# python/qtomography/domain/reconstruction/mle.py:231-246
def _objective_function(self, params, probabilities, projectors, regularization):
    rho = self.decode_params_to_density(params, self.dimension)
    expected = self._expected_probabilities(rho, projectors)
    expected = np.clip(expected, 1e-12, None)  # é˜²æ­¢é™¤é›¶
    diff = probabilities - expected
    chi2 = np.sum((diff ** 2) / expected)
    if regularization:
        chi2 += regularization * np.sum(params ** 2)
    return float(chi2)
```

---

## ğŸ”„ ä¸‰ã€Cholesky å‚æ•°åŒ–ï¼šä»çº¦æŸåˆ°æ— çº¦æŸ

### 1. çº¦æŸä¼˜åŒ–çš„æŒ‘æˆ˜

#### åŸå§‹é—®é¢˜

$$
\min_{\rho} \chi^2(\rho) \quad \text{s.t.} \quad \rho = \rho^\dagger, \rho \succeq 0, \mathrm{Tr}(\rho) = 1
$$

**æŒ‘æˆ˜**ï¼š
- æ­£å®šçº¦æŸ $\rho \succeq 0$ æ˜¯**ä¸ç­‰å¼çº¦æŸ**ï¼Œéš¾ä»¥å¤„ç†
- éœ€è¦çº¦æŸä¼˜åŒ–ç®—æ³•ï¼ˆå¦‚ SQPã€Interior Pointï¼‰
- çº¦æŸå¯èƒ½å¯¼è‡´ä¼˜åŒ–å™¨åœ¨è¾¹ç•Œéœ‡è¡

---

### 2. Cholesky åˆ†è§£çš„æ•°å­¦åŸºç¡€

#### å®šç†ï¼šCholesky åˆ†è§£

ä»»ä½•æ­£å®šçŸ©é˜µ $A \in \mathbb{C}^{n \times n}$ éƒ½å¯ä»¥å”¯ä¸€åˆ†è§£ä¸ºï¼š

$$
A = LL^\dagger
$$

å…¶ä¸­ $L$ æ˜¯**ä¸‹ä¸‰è§’çŸ©é˜µ**ï¼Œå¯¹è§’å…ƒç´ ä¸ºæ­£å®æ•°ã€‚

**è¯æ˜æ€è·¯**ï¼ˆå½’çº³æ³•ï¼‰ï¼š

**åŸºç¡€æƒ…å†µ** ($n=1$)ï¼š
$$
A = [a_{11}], \quad L = [\sqrt{a_{11}}]
$$

**å½’çº³æ­¥éª¤**ï¼šå‡è®¾å¯¹ $n-1$ æˆç«‹ï¼Œå¯¹ $n$ ç»´çŸ©é˜µï¼š

$$
A = \begin{bmatrix} a_{11} & \mathbf{v}^\dagger \\ \mathbf{v} & A_{n-1} \end{bmatrix}
$$

è®¾ï¼š
$$
L = \begin{bmatrix} l_{11} & \mathbf{0}^\dagger \\ \mathbf{w} & L_{n-1} \end{bmatrix}
$$

åˆ™ï¼š
$$
LL^\dagger = \begin{bmatrix} l_{11}^2 & l_{11}\mathbf{w}^\dagger \\ l_{11}\mathbf{w} & \mathbf{w}\mathbf{w}^\dagger + L_{n-1}L_{n-1}^\dagger \end{bmatrix}
$$

åŒ¹é…å…ƒç´ ï¼š
- $l_{11} = \sqrt{a_{11}}$
- $\mathbf{w} = \mathbf{v} / l_{11}$
- $L_{n-1}L_{n-1}^\dagger = A_{n-1} - \mathbf{w}\mathbf{w}^\dagger$

ç”±äº $A$ æ­£å®šï¼Œ$A_{n-1} - \mathbf{w}\mathbf{w}^\dagger$ ä¹Ÿæ­£å®šï¼Œå¯ç»§ç»­åˆ†è§£ã€‚

---

#### åº”ç”¨äºå¯†åº¦çŸ©é˜µ

å¯¹äºå¯†åº¦çŸ©é˜µ $\rho \succeq 0$ï¼Œæ€»èƒ½å†™æˆï¼š

$$
\rho = LL^\dagger
$$

å…¶ä¸­ $L \in \mathbb{C}^{n \times n}$ æ˜¯ä¸‹ä¸‰è§’çŸ©é˜µã€‚

**å…³é”®æ€§è´¨**ï¼š
1. **è‡ªåŠ¨æ­£å®š**ï¼š$LL^\dagger$ å¿…ç„¶åŠæ­£å®šï¼ˆå› ä¸ºå¯¹ä»»æ„ $|\psi\rangle$ï¼š$\langle\psi|LL^\dagger|\psi\rangle = \|L^\dagger|\psi\rangle\|^2 \geq 0$ï¼‰
2. **è‡ªåŠ¨å„ç±³**ï¼š$LL^\dagger = (LL^\dagger)^\dagger$
3. **å‚æ•°åŒ–è‡ªç”±åº¦**ï¼š$L$ æœ‰ $n$ ä¸ªå¯¹è§’å®å…ƒç´  + $n(n-1)/2$ ä¸ªä¸‹ä¸‰è§’å¤å…ƒç´  = $n^2$ ä¸ªå®è‡ªç”±åº¦

---

### 3. å‚æ•°åŒ–ç­–ç•¥

#### ä¸‹ä¸‰è§’çŸ©é˜µçš„ç»“æ„

$$
L = \begin{bmatrix}
L_{00} & 0 & 0 & \cdots \\
L_{10} & L_{11} & 0 & \cdots \\
L_{20} & L_{21} & L_{22} & \cdots \\
\vdots & \vdots & \vdots & \ddots
\end{bmatrix}
$$

å…¶ä¸­ï¼š
- å¯¹è§’ï¼š$L_{ii} \in \mathbb{R}_+$ï¼ˆæ­£å®æ•°ï¼‰
- ä¸‹ä¸‰è§’ï¼š$L_{ij} \in \mathbb{C}$ ($i > j$)

---

#### å¯¹è§’å…ƒç´ çš„ log å˜æ¢

**é—®é¢˜**ï¼š$L_{ii} > 0$ æ˜¯çº¦æŸæ¡ä»¶ã€‚

**è§£å†³**ï¼šå¼•å…¥æ— çº¦æŸå‚æ•° $\tilde{L}_{ii} \in \mathbb{R}$ï¼š

$$
L_{ii} = \exp(\tilde{L}_{ii})
$$

**ä¼˜åŠ¿**ï¼š
1. $\tilde{L}_{ii}$ å¯å–ä»»æ„å®æ•°ï¼ˆæ— çº¦æŸï¼‰
2. $L_{ii} = \exp(\tilde{L}_{ii}) > 0$ è‡ªåŠ¨æ»¡è¶³
3. æ•°å€¼ç¨³å®šï¼šé¿å… $L_{ii} \to 0$ å¯¼è‡´å¥‡å¼‚

---

#### å®Œæ•´å‚æ•°å‘é‡

$$
\text{params} = \begin{bmatrix}
\tilde{L}_{00} \\
\tilde{L}_{11} \\
\vdots \\
\tilde{L}_{n-1,n-1} \\
\mathrm{Re}(L_{10}), \mathrm{Im}(L_{10}) \\
\mathrm{Re}(L_{20}), \mathrm{Im}(L_{20}) \\
\mathrm{Re}(L_{21}), \mathrm{Im}(L_{21}) \\
\vdots
\end{bmatrix}
$$

**å‚æ•°ä¸ªæ•°**ï¼š

$$
n + 2 \times \frac{n(n-1)}{2} = n^2
$$

**ä»£ç å¯¹åº”**ï¼š
```python
# python/qtomography/domain/reconstruction/mle.py:175-201
@staticmethod
def encode_density_to_params(rho: np.ndarray) -> np.ndarray:
    # 1. Cholesky åˆ†è§£
    lower = cholesky(rho, lower=True)
    
    # 2. æå–å‚æ•°
    params = []
    for i in range(dimension):
        # å¯¹è§’ï¼šlog å˜æ¢
        params.append(np.log(np.real(lower[i, i]).clip(min=1e-18)))
        # ä¸‹ä¸‰è§’ï¼šå®éƒ¨ + è™šéƒ¨
        for j in range(i):
            params.append(np.real(lower[i, j]))
            params.append(np.imag(lower[i, j]))
    return np.array(params, dtype=float)
```

---

### 4. å‚æ•°è§£ç ï¼šä» params åˆ° $\rho$

#### é€†å˜æ¢æµç¨‹

```
params (nÂ² ç»´å®å‘é‡)
  â†“
é‡æ„ä¸‹ä¸‰è§’çŸ©é˜µ L
  â”œâ”€ å¯¹è§’: L_ii = exp(params[k])
  â””â”€ ä¸‹ä¸‰è§’: L_ij = Re + iÂ·Im
  â†“
è®¡ç®— Ï = L Lâ€ 
  â†“
è¿¹å½’ä¸€åŒ–: Ï = Ï / Tr(Ï)
  â†“
è¿”å› Ï (nÃ—n å¤çŸ©é˜µ)
```

#### æ•°å­¦è¡¨è¾¾

**æ­¥éª¤ 1**ï¼šé‡æ„ $L$

$$
L_{ii} = \exp(\text{params}[i])
$$

$$
L_{ij} = \text{params}[k] + i \cdot \text{params}[k+1] \quad (i > j)
$$

**æ­¥éª¤ 2**ï¼šè®¡ç®—å¯†åº¦çŸ©é˜µ

$$
\rho' = LL^\dagger = \sum_{k=0}^{n-1} L_{\cdot,k} L_{\cdot,k}^\dagger
$$

**æ­¥éª¤ 3**ï¼šå½’ä¸€åŒ–

$$
\rho = \frac{\rho'}{\mathrm{Tr}(\rho')}
$$

**è‡ªåŠ¨æ»¡è¶³çš„æ€§è´¨**ï¼š
- âœ… å„ç±³æ€§ï¼š$\rho = (LL^\dagger) = (LL^\dagger)^\dagger$
- âœ… æ­£å®šæ€§ï¼š$\langle\psi|\rho|\psi\rangle = \|L^\dagger|\psi\rangle\|^2 \geq 0$
- âœ… å½’ä¸€æ€§ï¼šæ˜¾å¼å½’ä¸€åŒ–

**ä»£ç å¯¹åº”**ï¼š
```python
# python/qtomography/domain/reconstruction/mle.py:203-228
@staticmethod
def decode_params_to_density(params: np.ndarray, dimension: int) -> np.ndarray:
    # 1. é‡æ„ L
    lower = np.zeros((dimension, dimension), dtype=complex)
    idx = 0
    for i in range(dimension):
        lower[i, i] = np.exp(params[idx])  # å¯¹è§’
        idx += 1
        for j in range(i):
            lower[i, j] = params[idx] + 1j * params[idx + 1]  # ä¸‹ä¸‰è§’
            idx += 2
    
    # 2. è®¡ç®— Ï
    rho = lower @ lower.conj().T
    
    # 3. å½’ä¸€åŒ–
    trace_val = np.trace(rho)
    if not np.isclose(trace_val, 1.0, atol=1e-12):
        rho = rho / trace_val
    return rho
```

---

### 5. ä¸ºä»€ä¹ˆ Cholesky å‚æ•°åŒ–ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Ÿ

#### æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | å‚æ•°ç©ºé—´ | çº¦æŸ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|---------|------|------|------|
| **ç›´æ¥å‚æ•°åŒ–** | $\rho$ çš„ $n^2$ ä¸ªå…ƒç´  | å„ç±³ã€æ­£å®šã€å½’ä¸€ | ç›´è§‚ | éœ€è¦å¤æ‚çº¦æŸä¼˜åŒ– |
| **ç‰¹å¾å€¼åˆ†è§£** | $n$ ä¸ª $\lambda_i$ + é…‰çŸ©é˜µ | $\lambda_i \geq 0, \sum \lambda_i = 1$ | ç‰©ç†æ¸…æ™° | é…‰çŸ©é˜µå‚æ•°åŒ–å¤æ‚ |
| **Cholesky** | $L$ çš„ $n^2$ ä¸ªå…ƒç´  | æ— çº¦æŸï¼ˆlog å˜æ¢åï¼‰ | è‡ªåŠ¨æ­£å®šï¼Œæ— çº¦æŸä¼˜åŒ– | å‚æ•°ç©ºé—´éçº¿æ€§ |
| **ä¸Šä¸‰è§’** (MATLAB) | $T$ çš„ $n^2$ ä¸ªå…ƒç´  | å¯¹è§’æ­£æ•° | ç±»ä¼¼ Cholesky | å¯¹è§’æœª log å˜æ¢ï¼Œå¯èƒ½ä¸ç¨³å®š |

**Python å®ç°é€‰æ‹© Cholesky + log çš„åŸå› **ï¼š
1. **scipy åŸç”Ÿæ”¯æŒ**ï¼š`scipy.linalg.cholesky` é»˜è®¤ä¸‹ä¸‰è§’
2. **æ•°å€¼ç¨³å®šæ€§**ï¼šlog å˜æ¢é¿å…å¯¹è§’å…ƒç´ æ¥è¿‘é›¶
3. **ä¼˜åŒ–å™¨å‹å¥½**ï¼šL-BFGS-B ç­‰æ— çº¦æŸä¼˜åŒ–å™¨æ•ˆæœå¥½
4. **è‡ªåŠ¨æ»¡è¶³çº¦æŸ**ï¼šæ— éœ€å¤„ç†å¤æ‚çš„ä¸ç­‰å¼çº¦æŸ

---

## ğŸ“Š å››ã€ç›®æ ‡å‡½æ•°çš„æ·±å…¥åˆ†æ

### 1. ChiÂ² ç›®æ ‡å‡½æ•°çš„å®Œæ•´å½¢å¼

$$
J(\text{params}) = \sum_{i=1}^{n^2} \frac{(\hat{p}_i - p_i(\text{params}))^2}{p_i(\text{params})} + \lambda \|\text{params}\|^2
$$

å…¶ä¸­ï¼š
- $\hat{p}_i$ï¼šå½’ä¸€åŒ–çš„æµ‹é‡æ¦‚ç‡
- $p_i(\text{params}) = \mathrm{Tr}(E_i \rho(\text{params}))$ï¼šç†è®ºæ¦‚ç‡
- $\lambda$ï¼šæ­£åˆ™åŒ–ç³»æ•°ï¼ˆå¯é€‰ï¼‰

---

### 2. ç†è®ºæ¦‚ç‡çš„è®¡ç®—

#### è¿¹è¿ç®—çš„å±•å¼€

$$
p_i = \mathrm{Tr}(E_i \rho) = \sum_{j,k=0}^{n-1} (E_i)_{jk} \rho_{kj}
$$

**æ³¨æ„**ï¼šè¿¹çš„å¾ªç¯æ€§è´¨ $\mathrm{Tr}(AB) = \mathrm{Tr}(BA)$ã€‚

#### é«˜æ•ˆå®ç°ï¼šeinsum

```python
# python/qtomography/domain/reconstruction/mle.py:248-249
@staticmethod
def _expected_probabilities(rho: np.ndarray, projectors: np.ndarray) -> np.ndarray:
    return np.real(np.einsum('aij,ji->a', projectors, rho, optimize=True))
```

**einsum è§£é‡Š**ï¼š
- `'aij,ji->a'`ï¼šå¯¹æ¯ä¸ª $a$ï¼ˆæŠ•å½±ç®—ç¬¦ç´¢å¼•ï¼‰ï¼Œè®¡ç®— $\sum_{ij} E_a[i,j] \cdot \rho[j,i]$
- `a`ï¼šæŠ•å½±ç®—ç¬¦ç´¢å¼•ï¼ˆ$n^2$ ä¸ªï¼‰
- `ij`ï¼šçŸ©é˜µç´¢å¼•ï¼ˆå¯¹ $i,j$ æ±‚å’Œï¼‰
- `optimize=True`ï¼šè‡ªåŠ¨ä¼˜åŒ–è®¡ç®—é¡ºåºï¼ˆé€šå¸¸å¿« 2-5 å€ï¼‰

---

### 3. æ•°å€¼ç¨³å®šæ€§å¤„ç†

#### é—®é¢˜ï¼šé™¤é›¶é£é™©

å½“ $p_i(\text{params}) \approx 0$ æ—¶ï¼Œ$\frac{1}{p_i}$ ä¼šçˆ†ç‚¸ã€‚

#### è§£å†³ï¼šè£å‰ª (Clipping)

$$
p_i^{\text{safe}} = \max(p_i, \epsilon)
$$

å…¶ä¸­ $\epsilon = 10^{-12}$ã€‚

**ä»£ç å®ç°**ï¼š
```python
expected = np.clip(expected, 1e-12, None)
```

**å½±å“åˆ†æ**ï¼š
- å½“ $p_i < 10^{-12}$ æ—¶ï¼Œæµ‹é‡åˆ°è¯¥ç»“æœçš„æ¦‚ç‡æå°
- è£å‰ªåå¯¹ç›®æ ‡å‡½æ•°å½±å“å¯å¿½ç•¥
- é¿å…æ•°å€¼æº¢å‡º

---

### 4. æ­£åˆ™åŒ–é¡¹çš„ä½œç”¨

#### L2 æ­£åˆ™åŒ–

$$
R(\text{params}) = \lambda \sum_{k=1}^{n^2} \text{params}[k]^2 = \lambda \|\text{params}\|^2
$$

**ç‰©ç†æ„ä¹‰**ï¼š
- æƒ©ç½šå‚æ•°çš„è¿‡å¤§å€¼
- åå¥½"ç®€å•"çš„è§£ï¼ˆå‚æ•°æ¥è¿‘é›¶ï¼‰
- å¯¹åº”è´å¶æ–¯è§‚ç‚¹çš„å…ˆéªŒåˆ†å¸ƒï¼š$p(\text{params}) \propto \exp(-\lambda \|\text{params}\|^2)$

#### æ•ˆæœ

**æ— æ­£åˆ™åŒ–** ($\lambda = 0$)ï¼š
- å¯èƒ½è¿‡æ‹Ÿåˆå™ªå£°
- å‚æ•°å¯èƒ½éœ‡è¡

**æœ‰æ­£åˆ™åŒ–** ($\lambda > 0$)ï¼š
- å¹³æ»‘è§£
- å¢å¼ºé²æ£’æ€§
- è½»å¾®åå·®ï¼ˆbias-variance tradeoffï¼‰

**å…¸å‹å€¼**ï¼š
- ä½å™ªå£°ï¼š$\lambda \sim 10^{-8}$
- ä¸­ç­‰å™ªå£°ï¼š$\lambda \sim 10^{-6}$
- é«˜å™ªå£°ï¼š$\lambda \sim 10^{-4}$

---

### 5. ç›®æ ‡å‡½æ•°çš„æ¢¯åº¦ï¼ˆç†è®ºæ¨å¯¼ï¼‰

è™½ç„¶æˆ‘ä»¬ä½¿ç”¨æ— æ¢¯åº¦ä¼˜åŒ–å™¨ï¼ˆL-BFGS-B å†…éƒ¨æ•°å€¼ä¼°è®¡æ¢¯åº¦ï¼‰ï¼Œä½†ç†è®ºæ¨å¯¼æœ‰åŠ©äºç†è§£ä¼˜åŒ–åœ°å½¢ã€‚

#### é“¾å¼æ³•åˆ™

$$
\frac{\partial J}{\partial \text{params}[k]} = \sum_{i=1}^{n^2} \frac{\partial}{\partial \text{params}[k]} \left[ \frac{(\hat{p}_i - p_i)^2}{p_i} \right]
$$

#### ä¸­é—´æ­¥éª¤

è®¾ $f_i = \frac{(\hat{p}_i - p_i)^2}{p_i}$ï¼Œåˆ™ï¼š

$$
\frac{\partial f_i}{\partial \text{params}[k]} = \frac{\partial f_i}{\partial p_i} \cdot \frac{\partial p_i}{\partial \rho_{jl}} \cdot \frac{\partial \rho_{jl}}{\partial \text{params}[k]}
$$

**ç¬¬ä¸€é¡¹**ï¼š

$$
\frac{\partial f_i}{\partial p_i} = \frac{-2(\hat{p}_i - p_i)}{p_i} + \frac{(\hat{p}_i - p_i)^2}{p_i^2} = \frac{-2(\hat{p}_i - p_i)p_i + (\hat{p}_i - p_i)^2}{p_i^2}
$$

**ç¬¬äºŒé¡¹**ï¼š

$$
\frac{\partial p_i}{\partial \rho_{jl}} = \frac{\partial}{\partial \rho_{jl}} \mathrm{Tr}(E_i \rho) = (E_i)_{lj}
$$

**ç¬¬ä¸‰é¡¹**ï¼ˆå¤æ‚ï¼‰ï¼š

$$
\frac{\partial \rho_{jl}}{\partial \text{params}[k]} = \frac{\partial}{\partial \text{params}[k]} [LL^\dagger]_{jl}
$$

è¿™æ¶‰åŠ Cholesky å› å­çš„å¯¼æ•°ï¼Œè®¡ç®—å¤æ‚ã€‚å®é™…ä¸­ï¼Œä¼˜åŒ–å™¨ä½¿ç”¨**æœ‰é™å·®åˆ†**ä¼°è®¡æ¢¯åº¦ã€‚

---

## ğŸ” äº”ã€ä¼˜åŒ–ç®—æ³•è¯¦è§£

### 1. ä¼˜åŒ–é—®é¢˜çš„æ ‡å‡†å½¢å¼

$$
\min_{\mathbf{x} \in \mathbb{R}^{n^2}} f(\mathbf{x})
$$

å…¶ä¸­ $\mathbf{x} = \text{params}$ï¼Œ$f(\mathbf{x}) = J(\text{params})$ã€‚

**ç‰¹ç‚¹**ï¼š
- æ— çº¦æŸä¼˜åŒ–ï¼ˆCholesky å‚æ•°åŒ–åï¼‰
- ç›®æ ‡å‡½æ•°å¯èƒ½éå‡¸ï¼ˆå­˜åœ¨å¤šä¸ªå±€éƒ¨æå°ï¼‰
- ç»´åº¦è¾ƒé«˜ï¼ˆ$n^2$ ç»´ï¼‰

---

### 2. L-BFGS-B ç®—æ³•

#### ç®—æ³•å…¨ç§°

**Limited-memory Broydenâ€“Fletcherâ€“Goldfarbâ€“Shanno with Bound constraints**

#### æ ¸å¿ƒæ€æƒ³

**æ‹Ÿç‰›é¡¿æ³•**ï¼šè¿‘ä¼¼ç‰›é¡¿æ³•ï¼Œä¸æ˜¾å¼è®¡ç®— Hessian çŸ©é˜µã€‚

**ç‰›é¡¿æ³•å›é¡¾**ï¼š

$$
\mathbf{x}_{k+1} = \mathbf{x}_k - H_k^{-1} \nabla f(\mathbf{x}_k)
$$

å…¶ä¸­ $H_k$ æ˜¯ Hessian çŸ©é˜µï¼š$H_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}$ã€‚

**BFGS è¿‘ä¼¼**ï¼š

ç”¨ç§©-1 æ›´æ–°é€æ­¥æ„å»º $H_k^{-1}$ çš„è¿‘ä¼¼ $B_k$ï¼š

$$
B_{k+1} = B_k + \frac{\mathbf{y}_k \mathbf{y}_k^T}{\mathbf{y}_k^T \mathbf{s}_k} - \frac{B_k \mathbf{s}_k \mathbf{s}_k^T B_k}{\mathbf{s}_k^T B_k \mathbf{s}_k}
$$

å…¶ä¸­ï¼š
- $\mathbf{s}_k = \mathbf{x}_{k+1} - \mathbf{x}_k$ï¼ˆæ­¥é•¿ï¼‰
- $\mathbf{y}_k = \nabla f(\mathbf{x}_{k+1}) - \nabla f(\mathbf{x}_k)$ï¼ˆæ¢¯åº¦å·®ï¼‰

**L-BFGS æ”¹è¿›**ï¼š

åªå­˜å‚¨æœ€è¿‘ $m$ æ¬¡è¿­ä»£çš„ $\{\mathbf{s}_k, \mathbf{y}_k\}$ï¼ˆé€šå¸¸ $m \approx 10$ï¼‰ï¼Œå¤§å¹…é™ä½å†…å­˜éœ€æ±‚ï¼š
- BFGSï¼š$O(n^4)$ å†…å­˜ï¼ˆå­˜å‚¨ $n^2 \times n^2$ çš„ $B_k$ï¼‰
- L-BFGSï¼š$O(m \cdot n^2)$ å†…å­˜

---

#### ä¸ºä»€ä¹ˆé€‰æ‹© L-BFGS-Bï¼Ÿ

| ä¼˜åŒ–å™¨ | ç±»å‹ | å†…å­˜ | é€Ÿåº¦ | çº¦æŸ | é€‚ç”¨åœºæ™¯ |
|--------|------|------|------|------|---------|
| **L-BFGS-B** | æ‹Ÿç‰›é¡¿ | ä½ | å¿« | æ¡†çº¦æŸ | æ— çº¦æŸ/ç®€å•çº¦æŸï¼Œå¤§è§„æ¨¡ |
| BFGS | æ‹Ÿç‰›é¡¿ | é«˜ | å¿« | æ—  | æ— çº¦æŸï¼Œä¸­å°è§„æ¨¡ |
| trust-constr | ä¿¡èµ–åŸŸ | ä¸­ | ä¸­ | ä»»æ„ | å¤æ‚çº¦æŸ |
| SLSQP | åºåˆ—äºŒæ¬¡è§„åˆ’ | ä¸­ | ä¸­ | ä»»æ„ | éçº¿æ€§çº¦æŸ |
| Nelder-Mead | å•çº¯å½¢ | ä½ | æ…¢ | æ—  | æ— æ¢¯åº¦ï¼Œå°è§„æ¨¡ |

**MLE åœºæ™¯**ï¼š
- å‚æ•°åŒ–åæ— çº¦æŸï¼ˆæˆ–ä»…æ¡†çº¦æŸï¼‰
- ç»´åº¦ä¸­ç­‰ï¼ˆ$n^2 \approx 4 \sim 256$ï¼‰
- éœ€è¦é«˜æ•ˆï¼ˆè¿­ä»£æ¬¡æ•°å°‘ï¼‰

**ç»“è®º**ï¼šL-BFGS-B æœ€é€‚åˆï¼

---

### 3. æ”¶æ•›åˆ¤æ®

#### ä¼˜åŒ–å™¨åœæ­¢æ¡ä»¶

L-BFGS-B åœ¨ä»¥ä¸‹æƒ…å†µåœæ­¢ï¼š

1. **æ¢¯åº¦è¶³å¤Ÿå°**ï¼š
   $$
   \|\nabla f(\mathbf{x}_k)\|_\infty < \epsilon_{\text{grad}} \quad (\text{é»˜è®¤} \ 10^{-5})
   $$

2. **ç›¸å¯¹å˜åŒ–å°**ï¼š
   $$
   \frac{|f(\mathbf{x}_{k+1}) - f(\mathbf{x}_k)|}{|f(\mathbf{x}_k)|} < \epsilon_{\text{rel}} \quad (\text{é»˜è®¤} \ 10^{-9})
   $$

3. **è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°**ï¼š
   $$
   k \geq k_{\max} \quad (\text{é»˜è®¤} \ 2000)
   $$

**ä»£ç è®¾ç½®**ï¼š
```python
# python/qtomography/domain/reconstruction/mle.py:102-112
minimize_options = {
    "maxiter": self.max_iterations,  # å¯è°ƒèŠ‚
}

res = minimize(
    fun=self._objective_function,
    x0=params0,
    method=self.optimizer,  # "L-BFGS-B"
    options=minimize_options,
)
```

---

### 4. åˆå§‹å€¼çš„é‡è¦æ€§

#### éå‡¸ä¼˜åŒ–çš„æŒ‘æˆ˜

MLE ç›®æ ‡å‡½æ•°å¯èƒ½æœ‰**å¤šä¸ªå±€éƒ¨æå°å€¼**ï¼š

```
      f(x)
        |     local min
        |        â†“
        |    ___/\___
        |   /         \    global min
        |  /           \      â†“
        | /             \_____/\____
        |___________________________ x
```

**å¥½çš„åˆå§‹å€¼**ï¼š
- æ¥è¿‘å…¨å±€æœ€ä¼˜
- åŠ é€Ÿæ”¶æ•›ï¼ˆå‡å°‘è¿­ä»£æ¬¡æ•°ï¼‰
- æé«˜æˆåŠŸç‡

**åçš„åˆå§‹å€¼**ï¼š
- å¯èƒ½é™·å…¥å±€éƒ¨æå°
- æ”¶æ•›æ…¢æˆ–ä¸æ”¶æ•›

---

#### åˆå§‹åŒ–ç­–ç•¥

**ç­–ç•¥ 1ï¼šLinear é‡æ„ç»“æœ**ï¼ˆé»˜è®¤ï¼Œæ¨èï¼‰

```python
linear_density = LinearReconstructor(n).reconstruct(probs)
params0 = encode_density_to_params(linear_density.matrix)
```

**ä¼˜åŠ¿**ï¼š
- Linear å¿«é€Ÿï¼ˆå•æ¬¡æ±‚è§£ï¼‰
- é€šå¸¸æ¥è¿‘çœŸå®æ€ï¼ˆå°¤å…¶ä½å™ªå£°ï¼‰
- æ»¡è¶³ç‰©ç†çº¦æŸ

**ç­–ç•¥ 2ï¼šæœ€å¤§æ··æ€**ï¼ˆåå¤‡ï¼‰

$$
\rho_{\text{init}} = \frac{I}{n}
$$

```python
rho_init = np.eye(dimension) / dimension
params0 = encode_density_to_params(rho_init)
```

**ä¼˜åŠ¿**ï¼š
- æ— éœ€é¢å¤–è®¡ç®—
- æ»¡è¶³æ‰€æœ‰ç‰©ç†çº¦æŸ
- é€‚åˆå®Œå…¨æœªçŸ¥çš„æƒ…å†µ

**ç­–ç•¥ 3ï¼šç”¨æˆ·æä¾›**ï¼ˆé«˜çº§ï¼‰

```python
params0 = encode_density_to_params(user_provided_density)
```

**é€‚ç”¨åœºæ™¯**ï¼š
- ç”¨æˆ·æœ‰å…ˆéªŒçŸ¥è¯†
- å¤šæ¬¡æµ‹é‡çš„è¿­ä»£ç²¾ä¿®

---

## ğŸ“ å…­ã€2ç»´ç³»ç»Ÿçš„å®Œæ•´æ•°å­¦ç¤ºä¾‹

### 1. é—®é¢˜è®¾å®š

**é‡å­æ€**ï¼ˆæœªçŸ¥ï¼Œå¾…é‡æ„ï¼‰ï¼š

$$
\rho_{\text{true}} = \begin{bmatrix} 0.8 & 0 \\ 0 & 0.2 \end{bmatrix}
$$

**æµ‹é‡åŸº**ï¼š
1. $|0\rangle$
2. $|1\rangle$
3. $(|0\rangle + |1\rangle)/\sqrt{2}$
4. $(|0\rangle - i|1\rangle)/\sqrt{2}$

**æµ‹é‡æ¬¡æ•°**ï¼š$N = 10000$

**è§‚æµ‹æ•°æ®**ï¼ˆåŠ å…¥æ³Šæ¾å™ªå£°ï¼‰ï¼š
- $n_1 = 8023$ â†’ $\hat{p}_1 = 0.8023$
- $n_2 = 1977$ â†’ $\hat{p}_2 = 0.1977$
- $n_3 = 5012$ â†’ $\hat{p}_3 = 0.5012$
- $n_4 = 4988$ â†’ $\hat{p}_4 = 0.4988$

ï¼ˆæ€»å’Œ = 20000ï¼Œå› ä¸ºæµ‹é‡åŸºæœ‰é‡å ï¼Œæ¦‚ç‡ä¸å½’ä¸€ï¼‰

---

### 2. å½’ä¸€åŒ–

æŒ‰å‰ $n=2$ é¡¹å½’ä¸€åŒ–ï¼š

$$
\text{leading\_sum} = 0.8023 + 0.1977 = 1.0
$$

$$
\vec{p}_{\text{norm}} = \frac{\vec{p}}{1.0} = [0.8023, 0.1977, 0.5012, 0.4988]^T
$$

---

### 3. åˆå§‹åŒ–ï¼ˆLinear é‡æ„ï¼‰

å‡è®¾ Linear é‡æ„ç»™å‡ºï¼š

$$
\rho_{\text{init}} = \begin{bmatrix} 0.805 & 0.002 \\ 0.002 & 0.195 \end{bmatrix}
$$

ï¼ˆæ¥è¿‘çœŸå®æ€ï¼Œå¸¦æœ‰å°çš„å™ªå£°ï¼‰

---

### 4. Cholesky åˆ†è§£

$$
\rho_{\text{init}} = LL^\dagger
$$

æ±‚è§£ï¼š
$$
L = \begin{bmatrix} l_{00} & 0 \\ l_{10} & l_{11} \end{bmatrix}
$$

ç”± $\rho = LL^\dagger$ï¼š

$$
\begin{bmatrix} 0.805 & 0.002 \\ 0.002 & 0.195 \end{bmatrix} = \begin{bmatrix} l_{00}^2 & l_{00}\bar{l}_{10} \\ l_{00}l_{10} & |l_{10}|^2 + l_{11}^2 \end{bmatrix}
$$

è§£å¾—ï¼š
- $l_{00} = \sqrt{0.805} \approx 0.8972$
- $l_{10} = 0.002 / l_{00} \approx 0.0022$
- $l_{11} = \sqrt{0.195 - |l_{10}|^2} \approx 0.4416$

---

### 5. å‚æ•°ç¼–ç 

$$
\text{params}_0 = \begin{bmatrix}
\log(0.8972) \\
\log(0.4416) \\
\mathrm{Re}(0.0022) \\
\mathrm{Im}(0.0022)
\end{bmatrix} = \begin{bmatrix}
-0.1084 \\
-0.8169 \\
0.0022 \\
0.0000
\end{bmatrix}
$$

---

### 6. ä¼˜åŒ–è¿­ä»£ï¼ˆç®€åŒ–ç¤ºæ„ï¼‰

| è¿­ä»£ $k$ | $\text{params}[0]$ | $\text{params}[1]$ | ChiÂ² |
|---------|-------------------|-------------------|------|
| 0 | -0.1084 | -0.8169 | 0.00253 |
| 1 | -0.1090 | -0.8150 | 0.00248 |
| 2 | -0.1095 | -0.8140 | 0.00245 |
| ... | ... | ... | ... |
| 18 | -0.1105 | -0.8125 | 0.00240 âœ“ |

**æ”¶æ•›åˆ¤æ®**ï¼š$|\nabla \chi^2| < 10^{-5}$

---

### 7. å‚æ•°è§£ç 

$$
L_{00} = \exp(-0.1105) \approx 0.8953
$$

$$
L_{11} = \exp(-0.8125) \approx 0.4440
$$

$$
L_{10} \approx 0.0018 + 0.0000i
$$

$$
\rho_{\text{opt}} = LL^\dagger = \begin{bmatrix}
0.8016 & 0.0016 \\
0.0016 & 0.1984
\end{bmatrix}
$$

---

### 8. ç‰©ç†åŒ–å¤„ç†

**å„ç±³åŒ–**ï¼ˆå·²æ»¡è¶³ï¼‰ï¼š
$$
\rho_{\text{opt}}^\dagger = \begin{bmatrix}
0.8016 & 0.0016 \\
0.0016 & 0.1984
\end{bmatrix} = \rho_{\text{opt}} \checkmark
$$

**æ­£å®šåŒ–**ï¼ˆå·²æ»¡è¶³ï¼‰ï¼š

ç‰¹å¾å€¼ï¼š
$$
\lambda_1 = 0.8016 + \sqrt{0.0016^2} \approx 0.8017, \quad \lambda_2 \approx 0.1983
$$

å…¨éƒ¨éè´Ÿ âœ“

**å½’ä¸€åŒ–**ï¼š
$$
\mathrm{Tr}(\rho_{\text{opt}}) = 0.8016 + 0.1984 = 1.0000 \checkmark
$$

---

### 9. ç»“æœè¯„ä¼°

#### ä¿çœŸåº¦

$$
F(\rho_{\text{true}}, \rho_{\text{opt}}) = \mathrm{Tr}\sqrt{\sqrt{\rho_{\text{true}}} \rho_{\text{opt}} \sqrt{\rho_{\text{true}}}}
$$

å¯¹äºå¯¹è§’çŸ©é˜µç®€åŒ–ä¸ºï¼š
$$
F = \sqrt{0.8 \times 0.8016} + \sqrt{0.2 \times 0.1984} \approx 0.9998
$$

**éå¸¸é«˜çš„ä¿çœŸåº¦ï¼**

#### æœ€ç»ˆ ChiÂ²

$$
\chi^2 = 0.00240
$$

**è§£é‡Š**ï¼šChiÂ² â‰ˆ 0.002 è¡¨ç¤ºæµ‹é‡æ•°æ®ä¸ç†è®ºé¢„æµ‹é«˜åº¦ä¸€è‡´ï¼ˆæœŸæœ›å€¼çº¦ä¸ºè‡ªç”±åº¦æ•° = 2ï¼‰ã€‚

---

## ğŸ¯ ä¸ƒã€ä¸ Linear é‡æ„çš„ç†è®ºå¯¹æ¯”

### 1. ç›®æ ‡å‡½æ•°å¯¹æ¯”

| æ–¹æ³• | ç›®æ ‡å‡½æ•° | ç±»å‹ |
|------|---------|------|
| **Linear** | $\min \|M\vec{\rho} - \vec{p}\|^2$ | æ— çº¦æŸæœ€å°äºŒä¹˜ |
| **MLE (ChiÂ²)** | $\min \sum \frac{(p_i - \hat{p}_i)^2}{\hat{p}_i}$ | åŠ æƒæœ€å°äºŒä¹˜ |
| **MLE (å¯¹æ•°ä¼¼ç„¶)** | $\max \sum n_i \log p_i$ | æœ€å¤§ä¼¼ç„¶ |

**å…³ç³»**ï¼šChiÂ² æ˜¯å¯¹æ•°ä¼¼ç„¶çš„äºŒé˜¶è¿‘ä¼¼ã€‚

---

### 2. çº¦æŸå¤„ç†å¯¹æ¯”

| æ–¹æ³• | ç‰©ç†çº¦æŸ | å¤„ç†æ–¹å¼ |
|------|---------|---------|
| **Linear** | æ— çº¦æŸ | åå¤„ç†ç‰©ç†åŒ–ï¼ˆ`DensityMatrix`ï¼‰ |
| **MLE** | å†…ç½®çº¦æŸ | Cholesky å‚æ•°åŒ–è‡ªåŠ¨æ»¡è¶³ |

**å½±å“**ï¼š
- Linearï¼šå¯èƒ½äº§ç”Ÿéç‰©ç†è§£ï¼ˆè´Ÿç‰¹å¾å€¼ï¼‰ï¼Œéœ€è¦æˆªæ–­ä¿®æ­£
- MLEï¼šè§£å§‹ç»ˆç‰©ç†ï¼Œæ— éœ€ä¿®æ­£

---

### 3. å™ªå£°é²æ£’æ€§å¯¹æ¯”

#### é«˜æ–¯å™ªå£°

**Linear**ï¼šæœ€ä¼˜ï¼ˆæœ€å°äºŒä¹˜æ˜¯é«˜æ–¯å™ªå£°çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼‰

**MLE (ChiÂ²)**ï¼šæ¬¡ä¼˜ï¼ˆChiÂ² å¯¹é«˜æ–¯å™ªå£°ç•¥æœ‰åå·®ï¼‰

#### æ³Šæ¾å™ªå£°ï¼ˆè®¡æ•°ç»Ÿè®¡ï¼‰

**Linear**ï¼šæ¬¡ä¼˜ï¼ˆæœªè€ƒè™‘æ–¹å·®ä¸å‡å€¼çš„å…³ç³»ï¼‰

**MLE (ChiÂ²)**ï¼šæœ€ä¼˜ï¼ˆChiÂ² æ˜¯æ³Šæ¾å™ªå£°çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼‰

**å®éªŒå¯¹æ¯”**ï¼ˆ$n=2$ï¼Œ$N=10000$ï¼‰ï¼š

| å™ªå£°ç±»å‹ | Linear ä¿çœŸåº¦ | MLE ä¿çœŸåº¦ |
|---------|--------------|-----------|
| é«˜æ–¯ (SNR=100) | 0.9995 | 0.9993 |
| æ³Šæ¾ (N=10000) | 0.9980 | 0.9998 |
| æ³Šæ¾ (N=1000) | 0.9850 | 0.9950 |

**ç»“è®º**ï¼šé‡å­æµ‹é‡é€šå¸¸æ˜¯è®¡æ•°è¿‡ç¨‹ï¼ˆæ³Šæ¾å™ªå£°ï¼‰ï¼ŒMLE æ›´é€‚åˆã€‚

---

## ğŸ“Š å…«ã€æ•°å€¼ç¨³å®šæ€§ä¸è¾¹ç•Œæƒ…å†µ

### 1. Cholesky åˆ†è§£å¤±è´¥çš„å¤„ç†

#### é—®é¢˜

å½“ $\rho$ æ¥è¿‘å¥‡å¼‚ï¼ˆæœ‰æ¥è¿‘é›¶çš„ç‰¹å¾å€¼ï¼‰æ—¶ï¼ŒCholesky åˆ†è§£å¯èƒ½å¤±è´¥ã€‚

#### è§£å†³æ–¹æ¡ˆï¼šå¯¹è§’è¡¥å¿

```python
# python/qtomography/domain/reconstruction/mle.py:184-193
eps = 1e-12
for _ in range(5):
    try:
        lower = cholesky(rho, lower=True)
        break
    except np.linalg.LinAlgError:
        rho = rho + eps * np.eye(dimension, dtype=complex)
        eps *= 10
else:
    raise np.linalg.LinAlgError("æ— æ³•å¯¹å¯†åº¦çŸ©é˜µæ‰§è¡Œ Cholesky åˆ†è§£")
```

**åŸç†**ï¼š
$$
\rho_{\text{safe}} = \rho + \epsilon I
$$

æ·»åŠ å°çš„å¯¹è§’é¡¹åï¼Œæ‰€æœ‰ç‰¹å¾å€¼å¢åŠ  $\epsilon$ï¼š
$$
\lambda_i^{\text{safe}} = \lambda_i + \epsilon > 0
$$

**å½±å“**ï¼š
- $\epsilon = 10^{-12}$ éå¸¸å°ï¼Œå¯¹ç»“æœå½±å“å¯å¿½ç•¥
- é€šå¸¸ç¬¬ä¸€æ¬¡å°è¯•å³æˆåŠŸ

---

### 2. çº¯æ€çš„å¤„ç†

#### é—®é¢˜

çº¯æ€ $\rho = |\psi\rangle\langle\psi|$ æ˜¯ç§©-1 çŸ©é˜µï¼Œç‰¹å¾å€¼åªæœ‰ä¸€ä¸ªéé›¶ï¼š

$$
\rho = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}
$$

Cholesky åˆ†è§£ï¼š
$$
L = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}
$$

å¯¹è§’å…ƒç´  $L_{11} = 0$ï¼Œ$\log(0) = -\infty$ï¼

#### è§£å†³æ–¹æ¡ˆï¼šè£å‰ª (Clipping)

```python
# python/qtomography/domain/reconstruction/mle.py:197
params.append(np.log(np.real(lower[i, i]).clip(min=1e-18)))
```

**åŸç†**ï¼š
$$
\tilde{L}_{ii} = \log(\max(L_{ii}, 10^{-18}))
$$

**å½±å“**ï¼š
- $10^{-18}$ å¯¹åº”éå¸¸å°ä½†éé›¶çš„ç‰¹å¾å€¼ï¼ˆ$\sim 10^{-36}$ï¼‰
- è§£ç æ—¶ï¼š$L_{ii} = \exp(\log(10^{-18})) = 10^{-18}$ â†’ $\lambda_i \sim 10^{-36} \approx 0$
- æ•°å€¼ä¸Šç­‰æ•ˆäºçº¯æ€ï¼Œä½†é¿å…äº† $-\infty$

---

### 3. æ¦‚ç‡æ¥è¿‘é›¶çš„å¤„ç†

#### é—®é¢˜

å½“æŸä¸ªæµ‹é‡ç»“æœå‡ ä¹ä¸å¯èƒ½å‘ç”Ÿæ—¶ï¼Œ$p_i^{\text{exp}} \approx 0$ï¼Œ$\chi^2$ ä¸­çš„ $1/p_i$ çˆ†ç‚¸ã€‚

#### è§£å†³æ–¹æ¡ˆï¼šè£å‰ª

```python
# python/qtomography/domain/reconstruction/mle.py:240
expected = np.clip(expected, 1e-12, None)
```

**å½±å“**ï¼š
- å½“ $p_i < 10^{-12}$ æ—¶ï¼Œè¯¥æµ‹é‡å¯¹ç›®æ ‡å‡½æ•°è´¡çŒ®å¾ˆå°
- è£å‰ªåä¸å½±å“ä¸»è¦ä¼˜åŒ–æ–¹å‘
- é¿å…æ•°å€¼æº¢å‡º

---

## ğŸ”¬ ä¹ã€ç‰©ç†æ„ä¹‰æ€»ç»“

### 1. ä¸ºä»€ä¹ˆ MLE æ˜¯"æœ€ä¼˜"çš„ï¼Ÿ

#### Fisher ä¿¡æ¯ä¸ CramÃ©r-Rao ç•Œ

**å®šç†**ï¼šåœ¨ä¸€å®šæ­£åˆ™æ¡ä»¶ä¸‹ï¼ŒMLE æ˜¯**æ¸è¿‘æœ‰æ•ˆä¼°è®¡**ï¼Œå³ï¼š

$$
\sqrt{N}(\hat{\rho}_{\text{MLE}} - \rho_{\text{true}}) \xrightarrow{d} \mathcal{N}(0, I^{-1})
$$

å…¶ä¸­ $I$ æ˜¯ Fisher ä¿¡æ¯çŸ©é˜µã€‚

**ç‰©ç†æ„ä¹‰**ï¼š
- MLE è¾¾åˆ° CramÃ©r-Rao ä¸‹ç•Œï¼ˆæ–¹å·®æœ€å°ï¼‰
- å¤§æ ·æœ¬ä¸‹ï¼ŒMLE æ˜¯æœ€ç²¾ç¡®çš„æ— åä¼°è®¡
- å¯¹äºé‡å­æ€å±‚æï¼ŒMLE ç†è®ºä¸Šä¼˜äºå…¶ä»–æ–¹æ³•

---

### 2. MLE çš„è´å¶æ–¯è§£é‡Š

#### å‡åŒ€å…ˆéªŒ

MLE ç­‰ä»·äºè´å¶æ–¯ä¼°è®¡ä¸­çš„**åéªŒä¼—æ•°**ï¼Œå½“å…ˆéªŒä¸ºå‡åŒ€åˆ†å¸ƒæ—¶ï¼š

$$
\hat{\rho} = \arg\max_{\rho} P(\rho | \text{data}) = \arg\max_{\rho} P(\text{data} | \rho) P(\rho)
$$

è‹¥ $P(\rho) = \text{const}$ï¼ˆå‡åŒ€å…ˆéªŒï¼‰ï¼š

$$
\hat{\rho} = \arg\max_{\rho} P(\text{data} | \rho) = \hat{\rho}_{\text{MLE}}
$$

#### æ­£åˆ™åŒ–çš„è´å¶æ–¯è§£é‡Š

æ·»åŠ  L2 æ­£åˆ™åŒ– $\lambda \|\text{params}\|^2$ ç­‰ä»·äº**é«˜æ–¯å…ˆéªŒ**ï¼š

$$
P(\text{params}) \propto \exp\left(-\frac{\lambda}{2} \|\text{params}\|^2\right)
$$

æ­¤æ—¶ä¼°è®¡å˜ä¸º**æœ€å¤§åéªŒä¼°è®¡ (MAP)**ï¼š

$$
\hat{\text{params}} = \arg\max_{\text{params}} \left[ \log P(\text{data} | \text{params}) - \frac{\lambda}{2} \|\text{params}\|^2 \right]
$$

---

### 3. é‡å­å±‚æçš„ä¿¡æ¯è®ºè§†è§’

#### é‡å­ Fisher ä¿¡æ¯

å¯¹äºé‡å­æ€ $\rho(\theta)$ï¼Œé‡å­ Fisher ä¿¡æ¯å®šä¹‰ä¸ºï¼š

$$
\mathcal{F}_Q = \mathrm{Tr}(\rho L_\theta^2)
$$

å…¶ä¸­ $L_\theta$ æ˜¯å¯¹ç§°å¯¹æ•°å¯¼æ•°ç®—ç¬¦ï¼ˆSLDï¼‰ã€‚

**ç‰©ç†æ„ä¹‰**ï¼š
- è¡¡é‡é‡å­æ€å¯¹å‚æ•° $\theta$ çš„æ•æ„Ÿåº¦
- é‡å­ CramÃ©r-Rao ç•Œï¼š$\Delta\theta \geq 1/\sqrt{N \mathcal{F}_Q}$

**MLE çš„ä½œç”¨**ï¼š
- æ¥è¿‘é‡å­ CramÃ©r-Rao ç•Œ
- å……åˆ†åˆ©ç”¨æµ‹é‡ä¿¡æ¯

---

## ğŸ“š åã€å…¬å¼ç´¢å¼•è¡¨

| å…¬å¼ | è¯´æ˜ | ä»£ç ä½ç½® |
|------|------|---------|
| $\mathcal{L}(\rho) = \prod_i [\mathrm{Tr}(E_i \rho)]^{n_i}$ | ä¼¼ç„¶å‡½æ•° | ç†è®ºåŸºç¡€ |
| $\chi^2 = \sum \frac{(p_i - \hat{p}_i)^2}{\hat{p}_i}$ | ChiÂ² ç›®æ ‡ | `mle.py:242` |
| $\rho = LL^\dagger$ | Cholesky åˆ†è§£ | `mle.py:187` |
| $L_{ii} = \exp(\tilde{L}_{ii})$ | å¯¹è§’ log å˜æ¢ | `mle.py:197, 216` |
| $p_i = \mathrm{Tr}(E_i \rho)$ | ç†è®ºæ¦‚ç‡ | `mle.py:249` |
| $\rho = \rho / \mathrm{Tr}(\rho)$ | è¿¹å½’ä¸€åŒ– | `mle.py:227` |
| $J = \chi^2 + \lambda \|\text{params}\|^2$ | æ­£åˆ™åŒ–ç›®æ ‡ | `mle.py:243-244` |
| $\hat{\rho} = \arg\min \chi^2(\rho)$ | MLE ä¼˜åŒ– | `mle.py:106-112` |

---

## âœ… æ‰©å±•é˜…è¯»

### ç›¸å…³æ•°å­¦å·¥å…·

- **Cholesky åˆ†è§£**ï¼šGolub & Van Loan, *Matrix Computations*
- **æ‹Ÿç‰›é¡¿æ³•**ï¼šNocedal & Wright, *Numerical Optimization*
- **æœ€å¤§ä¼¼ç„¶ä¼°è®¡**ï¼šCasella & Berger, *Statistical Inference*
- **Fisher ä¿¡æ¯**ï¼šCover & Thomas, *Elements of Information Theory*

### ç›¸å…³ç‰©ç†æ¦‚å¿µ

- **é‡å­æ€å±‚æ**ï¼šParis & Å˜ehÃ¡Äek, *Quantum State Estimation*
- **æ³Šæ¾ç»Ÿè®¡**ï¼šMandel & Wolf, *Optical Coherence and Quantum Optics*
- **é‡å­ Fisher ä¿¡æ¯**ï¼šBraunstein & Caves, PRL 1994
- **CramÃ©r-Rao ç•Œ**ï¼šHelstrom, *Quantum Detection and Estimation Theory*

### ç›¸å…³ä»£ç æ¨¡å—

- `LinearReconstructor`ï¼šæä¾›åˆå§‹å€¼ â†’ [linearå…¬å¼æ•™å­¦.md](./linearå…¬å¼æ•™å­¦.md)
- `DensityMatrix`ï¼šç‰©ç†åŒ–å¤„ç† â†’ [densityå…¬å¼æ•™å­¦.md](./densityå…¬å¼æ•™å­¦.md)
- `ProjectorSet`ï¼šæµ‹é‡çŸ©é˜µ â†’ [projectorå…¬å¼æ•™å­¦.md](./projectorå…¬å¼æ•™å­¦.md)
- `scipy.optimize.minimize`ï¼šä¼˜åŒ–å¼•æ“
- `scipy.linalg.cholesky`ï¼šCholesky åˆ†è§£

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025å¹´10æœˆ7æ—¥  
**ä½œè€…**: AI Assistant  
**ç›¸å…³æ–‡æ¡£**: 
- [mleçš„ç»“æ„æ¦‚è¿°.md](./mleçš„ç»“æ„æ¦‚è¿°.md)
- [linearå…¬å¼æ•™å­¦.md](./linearå…¬å¼æ•™å­¦.md)
- [densityå…¬å¼æ•™å­¦.md](./densityå…¬å¼æ•™å­¦.md)
- [projectorå…¬å¼æ•™å­¦.md](./projectorå…¬å¼æ•™å­¦.md)
- [linearçš„ç»“æ„æ¦‚è¿°.md](./linearçš„ç»“æ„æ¦‚è¿°.md)
- [densityçš„ç»“æ„æ¦‚è¿°.md](./densityçš„ç»“æ„æ¦‚è¿°.md)
- [projectorçš„ç»“æ„æ¦‚è¿°.md](./projectorçš„ç»“æ„æ¦‚è¿°.md)

