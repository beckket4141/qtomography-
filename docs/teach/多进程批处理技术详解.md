# é‡å­å±‚ææ‰¹å¤„ç†å¤šè¿›ç¨‹æŠ€æœ¯è¯¦è§£

## ğŸ“‹ ç›®å½•
- [æ¦‚è¿°](#æ¦‚è¿°)
- [æŠ€æœ¯æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ](#æŠ€æœ¯æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ)
- [å®ç°æ–¹æ¡ˆ](#å®ç°æ–¹æ¡ˆ)
- [æ€§èƒ½åˆ†æ](#æ€§èƒ½åˆ†æ)
- [é¢è¯•é—®ç­”å‡†å¤‡](#é¢è¯•é—®ç­”å‡†å¤‡)
- [ä»£ç ç¤ºä¾‹](#ä»£ç ç¤ºä¾‹)

---

## æ¦‚è¿°

### èƒŒæ™¯
åœ¨é‡å­å±‚æé‡æ„çš„æ‰¹å¤„ç†ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å¤„ç†å¤§é‡æ ·æœ¬ï¼ˆé€šå¸¸å‡ ç™¾åˆ°å‡ åƒä¸ªï¼‰ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½éœ€è¦æ‰§è¡Œï¼š
- çº¿æ€§é‡æ„ï¼ˆLinear Reconstructionï¼‰
- MLEé‡æ„ï¼ˆMaximum Likelihood Estimationï¼‰
- ç»“æœåˆ†æå’ŒæŒ‡æ ‡è®¡ç®—

### ä¸ºä»€ä¹ˆéœ€è¦å¤šè¿›ç¨‹ï¼Ÿ
1. **è®¡ç®—å¯†é›†å‹ä»»åŠ¡**ï¼šæ¯ä¸ªæ ·æœ¬çš„MLEé‡æ„æ¶‰åŠå¤§é‡æ•°å€¼ä¼˜åŒ–è®¡ç®—
2. **CPUåˆ©ç”¨ç‡**ï¼šç°ä»£å¤šæ ¸CPUå¯ä»¥å¹¶è¡Œå¤„ç†å¤šä¸ªæ ·æœ¬
3. **æ—¶é—´æ•ˆç‡**ï¼šæ‰¹å¤„ç†æ—¶é—´ä»O(n)é™ä½åˆ°O(n/p)ï¼Œå…¶ä¸­pæ˜¯è¿›ç¨‹æ•°

### å½“å‰å®ç° vs å¤šè¿›ç¨‹å®ç°
```python
# å½“å‰ï¼šå•çº¿ç¨‹é¡ºåºå¤„ç†
for idx in range(sample_count):
    process_sample(idx, data[:, idx])

# å¤šè¿›ç¨‹ï¼šå¹¶è¡Œå¤„ç†
with ProcessPoolExecutor(max_workers=cpu_count) as executor:
    futures = [executor.submit(process_sample, idx, data[:, idx]) 
               for idx in range(sample_count)]
```

---

## æŠ€æœ¯æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ

### 1. è¿›ç¨‹é—´é€šä¿¡é—®é¢˜

#### é—®é¢˜æè¿°
- é‡æ„å™¨å¯¹è±¡ï¼ˆLinearReconstructor, MLEReconstructorï¼‰æ— æ³•ç›´æ¥ä¼ é€’ç»™å­è¿›ç¨‹
- æŠ•å½±ç®—å­çŸ©é˜µéœ€è¦åœ¨è¿›ç¨‹é—´å…±äº«
- é…ç½®å¯¹è±¡éœ€è¦åºåˆ—åŒ–ä¼ é€’

#### è§£å†³æ–¹æ¡ˆ
```python
# æ–¹æ¡ˆ1ï¼šå‡½æ•°å¼è®¾è®¡
def process_single_sample(args):
    """ç‹¬ç«‹çš„æ ·æœ¬å¤„ç†å‡½æ•°ï¼Œä¸ä¾èµ–ç±»å®ä¾‹"""
    idx, probs, config_dict, dimension = args
    
    # åœ¨å­è¿›ç¨‹ä¸­é‡æ–°åˆ›å»ºé‡æ„å™¨
    linear = LinearReconstructor(dimension, **config_dict['linear_params'])
    mle = MLEReconstructor(dimension, **config_dict['mle_params'])
    
    # æ‰§è¡Œé‡æ„...
    return result

# æ–¹æ¡ˆ2ï¼šä½¿ç”¨å…±äº«å†…å­˜
from multiprocessing import shared_memory
import numpy as np

def create_shared_projectors(projectors):
    """åˆ›å»ºå…±äº«å†…å­˜å­˜å‚¨æŠ•å½±ç®—å­"""
    shm = shared_memory.SharedMemory(create=True, size=projectors.nbytes)
    shared_array = np.ndarray(projectors.shape, dtype=projectors.dtype, buffer=shm.buf)
    shared_array[:] = projectors[:]
    return shm.name, projectors.shape, projectors.dtype
```

### 2. å†…å­˜ç®¡ç†é—®é¢˜

#### é—®é¢˜æè¿°
- æ¯ä¸ªè¿›ç¨‹éƒ½ä¼šå¤åˆ¶ä¸€ä»½æ•°æ®
- æŠ•å½±ç®—å­çŸ©é˜µå¯èƒ½å¾ˆå¤§ï¼ˆdÂ²Ã—dÂ²ï¼‰
- ç»“æœæ•°æ®éœ€è¦æ”¶é›†å’Œåˆå¹¶

#### è§£å†³æ–¹æ¡ˆ
```python
# å†…å­˜ä¼˜åŒ–ç­–ç•¥
class MemoryEfficientBatchProcessor:
    def __init__(self, max_workers=None):
        self.max_workers = max_workers or min(mp.cpu_count(), 8)
        self.shared_objects = {}
    
    def precompute_shared_objects(self, dimension, config):
        """é¢„è®¡ç®—å…±äº«å¯¹è±¡ï¼Œé¿å…é‡å¤è®¡ç®—"""
        # è®¡ç®—æŠ•å½±ç®—å­ï¼ˆåªè®¡ç®—ä¸€æ¬¡ï¼‰
        projectors = self._compute_projectors(dimension)
        
        # å­˜å‚¨åˆ°å…±äº«å†…å­˜
        shm_name, shape, dtype = create_shared_projectors(projectors)
        self.shared_objects['projectors'] = (shm_name, shape, dtype)
        
        # åºåˆ—åŒ–é…ç½®
        self.shared_objects['config'] = self._serialize_config(config)
    
    def cleanup_shared_objects(self):
        """æ¸…ç†å…±äº«å†…å­˜"""
        for obj in self.shared_objects.values():
            if isinstance(obj, tuple) and len(obj) == 3:
                shm = shared_memory.SharedMemory(name=obj[0])
                shm.close()
                shm.unlink()
```

### 3. é”™è¯¯å¤„ç†å’Œå®¹é”™

#### é—®é¢˜æè¿°
- å•ä¸ªæ ·æœ¬å¤±è´¥ä¸åº”å½±å“æ•´ä¸ªæ‰¹å¤„ç†
- éœ€è¦æ”¶é›†å’ŒæŠ¥å‘Šé”™è¯¯ä¿¡æ¯
- éƒ¨åˆ†ç»“æœéœ€è¦ä¿å­˜

#### è§£å†³æ–¹æ¡ˆ
```python
def robust_batch_processing(sample_data, config, max_workers=4):
    """å®¹é”™çš„æ‰¹å¤„ç†å®ç°"""
    results = []
    errors = []
    
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_idx = {
            executor.submit(process_single_sample, args): idx 
            for idx, args in enumerate(sample_data)
        }
        
        # æ”¶é›†ç»“æœ
        for future in as_completed(future_to_idx):
            idx = future_to_idx[future]
            try:
                result = future.result(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
                results.append((idx, result))
            except Exception as e:
                error_info = {
                    'sample_index': idx,
                    'error_type': type(e).__name__,
                    'error_message': str(e),
                    'timestamp': datetime.now().isoformat()
                }
                errors.append(error_info)
                print(f"æ ·æœ¬ {idx} å¤„ç†å¤±è´¥: {e}")
    
    return results, errors
```

### 4. è¿›åº¦ç›‘æ§å’Œæ—¥å¿—

#### é—®é¢˜æè¿°
- æ‰¹å¤„ç†æ—¶é—´è¾ƒé•¿ï¼Œéœ€è¦è¿›åº¦æ˜¾ç¤º
- å¤šè¿›ç¨‹ç¯å¢ƒä¸‹æ—¥å¿—å¯èƒ½æ··ä¹±
- éœ€è¦å®æ—¶ç›‘æ§å¤„ç†çŠ¶æ€

#### è§£å†³æ–¹æ¡ˆ
```python
import logging
from tqdm import tqdm
import threading

class ProgressTracker:
    def __init__(self, total_samples):
        self.total = total_samples
        self.completed = 0
        self.lock = threading.Lock()
        self.pbar = tqdm(total=total_samples, desc="æ‰¹å¤„ç†è¿›åº¦")
    
    def update(self, increment=1):
        with self.lock:
            self.completed += increment
            self.pbar.update(increment)
    
    def close(self):
        self.pbar.close()

def process_with_progress(sample_data, config):
    """å¸¦è¿›åº¦ç›‘æ§çš„æ‰¹å¤„ç†"""
    tracker = ProgressTracker(len(sample_data))
    
    def process_with_tracking(args):
        try:
            result = process_single_sample(args)
            tracker.update(1)
            return result
        except Exception as e:
            tracker.update(1)  # å³ä½¿å¤±è´¥ä¹Ÿè¦æ›´æ–°è¿›åº¦
            raise e
    
    # æ‰§è¡Œæ‰¹å¤„ç†...
    tracker.close()
```

### 5. æ•°æ®ä¸€è‡´æ€§å’ŒåŒæ­¥

#### é—®é¢˜æè¿°
- å¤šä¸ªè¿›ç¨‹åŒæ—¶å†™å…¥æ–‡ä»¶å¯èƒ½å†²çª
- ç»“æœæ”¶é›†éœ€è¦ä¿è¯é¡ºåº
- å…ƒæ•°æ®éœ€è¦æ­£ç¡®å…³è”

#### è§£å†³æ–¹æ¡ˆ
```python
import tempfile
import json
from pathlib import Path

class AtomicResultWriter:
    """åŸå­æ€§ç»“æœå†™å…¥å™¨"""
    def __init__(self, output_dir):
        self.output_dir = Path(output_dir)
        self.temp_dir = Path(tempfile.mkdtemp())
        self.lock = threading.Lock()
    
    def write_result(self, sample_idx, method, result):
        """åŸå­æ€§å†™å…¥å•ä¸ªç»“æœ"""
        temp_file = self.temp_dir / f"{sample_idx}_{method}.json"
        
        # å†™å…¥ä¸´æ—¶æ–‡ä»¶
        with open(temp_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        # åŸå­æ€§ç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®
        target_file = self.output_dir / f"{sample_idx}_{method}.json"
        with self.lock:
            temp_file.rename(target_file)
    
    def finalize(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)
```

---

## å®ç°æ–¹æ¡ˆ

### æ–¹æ¡ˆ1ï¼šåŸºäºProcessPoolExecutorçš„å®ç°

```python
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing as mp
from typing import List, Dict, Any, Tuple
import pickle
import tempfile

class MultiProcessBatchController:
    """å¤šè¿›ç¨‹æ‰¹å¤„ç†æ§åˆ¶å™¨"""
    
    def __init__(self, max_workers: int = None, chunk_size: int = 1):
        self.max_workers = max_workers or min(mp.cpu_count(), 8)
        self.chunk_size = chunk_size
        self.temp_dir = None
    
    def run_batch(self, config: ReconstructionConfig) -> SummaryResult:
        """å¤šè¿›ç¨‹æ‰¹å¤„ç†ä¸»æ–¹æ³•"""
        # 1. å‡†å¤‡é˜¶æ®µ
        data = self._load_and_prepare_data(config)
        shared_objects = self._prepare_shared_objects(config, data.shape[0])
        
        # 2. åˆ›å»ºä¸´æ—¶ç›®å½•
        self.temp_dir = Path(tempfile.mkdtemp())
        
        try:
            # 3. å¤šè¿›ç¨‹å¤„ç†
            results = self._process_samples_parallel(
                data, config, shared_objects
            )
            
            # 4. æ”¶é›†å’Œæ±‡æ€»ç»“æœ
            return self._collect_and_summarize(results, config)
            
        finally:
            # 5. æ¸…ç†
            self._cleanup(shared_objects)
            if self.temp_dir.exists():
                shutil.rmtree(self.temp_dir)
    
    def _process_samples_parallel(self, data, config, shared_objects):
        """å¹¶è¡Œå¤„ç†æ ·æœ¬"""
        sample_args = self._prepare_sample_args(data, config, shared_objects)
        
        results = []
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤ä»»åŠ¡
            future_to_idx = {
                executor.submit(process_single_sample_worker, args): idx
                for idx, args in enumerate(sample_args)
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_idx):
                idx = future_to_idx[future]
                try:
                    result = future.result(timeout=600)  # 10åˆ†é’Ÿè¶…æ—¶
                    results.append((idx, result))
                except Exception as e:
                    print(f"æ ·æœ¬ {idx} å¤„ç†å¤±è´¥: {e}")
                    results.append((idx, None))
        
        return results

def process_single_sample_worker(args):
    """å·¥ä½œè¿›ç¨‹å‡½æ•°ï¼ˆå¿…é¡»å®šä¹‰åœ¨æ¨¡å—çº§åˆ«ï¼‰"""
    (idx, probs, config_dict, shared_objects, temp_dir) = args
    
    try:
        # é‡å»ºé‡æ„å™¨
        linear = LinearReconstructor(**config_dict['linear_params'])
        mle = MLEReconstructor(**config_dict['mle_params'])
        
        # æ‰§è¡Œé‡æ„
        results = {}
        
        # çº¿æ€§é‡æ„
        if 'linear' in config_dict['methods']:
            linear_result = linear.reconstruct_with_details(probs)
            results['linear'] = _serialize_result(linear_result)
        
        # MLEé‡æ„
        if 'mle' in config_dict['methods']:
            initial_density = results.get('linear', {}).get('density_matrix')
            mle_result = mle.reconstruct_with_details(probs, initial_density)
            results['mle'] = _serialize_result(mle_result)
        
        # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        result_file = Path(temp_dir) / f"sample_{idx}.pkl"
        with open(result_file, 'wb') as f:
            pickle.dump(results, f)
        
        return str(result_file)
        
    except Exception as e:
        # ä¿å­˜é”™è¯¯ä¿¡æ¯
        error_file = Path(temp_dir) / f"error_{idx}.pkl"
        with open(error_file, 'wb') as f:
            pickle.dump({'error': str(e), 'type': type(e).__name__}, f)
        return str(error_file)
```

### æ–¹æ¡ˆ2ï¼šåŸºäºmultiprocessing.Poolçš„å®ç°

```python
import multiprocessing as mp
from multiprocessing import Pool, Manager

class PoolBasedBatchController:
    """åŸºäºPoolçš„å¤šè¿›ç¨‹æ‰¹å¤„ç†"""
    
    def __init__(self, processes=None):
        self.processes = processes or mp.cpu_count()
        self.manager = Manager()
    
    def run_batch(self, config: ReconstructionConfig) -> SummaryResult:
        """ä½¿ç”¨Poolçš„æ‰¹å¤„ç†å®ç°"""
        # å‡†å¤‡æ•°æ®
        data = self._load_data(config)
        
        # åˆ›å»ºå…±äº«å¯¹è±¡
        shared_dict = self.manager.dict()
        self._setup_shared_objects(shared_dict, config)
        
        # å‡†å¤‡å‚æ•°
        sample_args = [
            (idx, data[:, idx], shared_dict, config)
            for idx in range(data.shape[1])
        ]
        
        # ä½¿ç”¨Poolå¤„ç†
        with Pool(processes=self.processes) as pool:
            results = pool.map(process_sample_pool_worker, sample_args)
        
        # æ”¶é›†ç»“æœ
        return self._collect_results(results, config)

def process_sample_pool_worker(args):
    """Poolå·¥ä½œå‡½æ•°"""
    idx, probs, shared_dict, config = args
    
    # ä»å…±äº«å­—å…¸è·å–é…ç½®
    linear_params = shared_dict['linear_params']
    mle_params = shared_dict['mle_params']
    
    # åˆ›å»ºé‡æ„å™¨
    linear = LinearReconstructor(**linear_params)
    mle = MLEReconstructor(**mle_params)
    
    # æ‰§è¡Œé‡æ„...
    return process_and_serialize(probs, linear, mle, config)
```

---

## æ€§èƒ½åˆ†æ

### ç†è®ºåŠ é€Ÿæ¯”
```
ç†è®ºåŠ é€Ÿæ¯” = min(è¿›ç¨‹æ•°, CPUæ ¸å¿ƒæ•°, æ ·æœ¬æ•°)
å®é™…åŠ é€Ÿæ¯” = ç†è®ºåŠ é€Ÿæ¯” Ã— å¹¶è¡Œæ•ˆç‡

å…¶ä¸­ï¼š
- å¹¶è¡Œæ•ˆç‡ = 1 - (é€šä¿¡å¼€é”€ + åŒæ­¥å¼€é”€) / æ€»æ—¶é—´
- é€šä¿¡å¼€é”€ = æ•°æ®ä¼ è¾“æ—¶é—´ + åºåˆ—åŒ–/ååºåˆ—åŒ–æ—¶é—´
- åŒæ­¥å¼€é”€ = è¿›ç¨‹åˆ›å»º/é”€æ¯æ—¶é—´ + ç­‰å¾…æ—¶é—´
```

### æ€§èƒ½æµ‹è¯•ç»“æœï¼ˆæ¨¡æ‹Ÿï¼‰
```
æ ·æœ¬æ•°: 1000
CPUæ ¸å¿ƒ: 8
å•è¿›ç¨‹æ—¶é—´: 120ç§’
å¤šè¿›ç¨‹æ—¶é—´: 18ç§’
åŠ é€Ÿæ¯”: 6.7x
å¹¶è¡Œæ•ˆç‡: 83.75%
```

### å†…å­˜ä½¿ç”¨åˆ†æ
```
å•è¿›ç¨‹å†…å­˜: åŸºç¡€å†…å­˜ + æ•°æ®å†…å­˜
å¤šè¿›ç¨‹å†…å­˜: åŸºç¡€å†…å­˜ Ã— è¿›ç¨‹æ•° + æ•°æ®å†…å­˜ Ã— è¿›ç¨‹æ•° + å…±äº«å†…å­˜

ä¼˜åŒ–ç­–ç•¥:
1. ä½¿ç”¨å…±äº«å†…å­˜å­˜å‚¨æŠ•å½±ç®—å­
2. æµå¼å¤„ç†å¤§æ•°æ®é›†
3. åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„æ•°æ®
```

---

## é¢è¯•é—®ç­”å‡†å¤‡

### Q1: ä¸ºä»€ä¹ˆé€‰æ‹©å¤šè¿›ç¨‹è€Œä¸æ˜¯å¤šçº¿ç¨‹ï¼Ÿ

**å›ç­”è¦ç‚¹ï¼š**
1. **GILé™åˆ¶**ï¼šPythonçš„å…¨å±€è§£é‡Šå™¨é”é™åˆ¶äº†å¤šçº¿ç¨‹åœ¨CPUå¯†é›†å‹ä»»åŠ¡ä¸­çš„æ€§èƒ½
2. **è®¡ç®—ç‰¹æ€§**ï¼šé‡å­å±‚æé‡æ„æ˜¯CPUå¯†é›†å‹ä»»åŠ¡ï¼Œå¤šè¿›ç¨‹èƒ½å……åˆ†åˆ©ç”¨å¤šæ ¸CPU
3. **å†…å­˜éš”ç¦»**ï¼šæ¯ä¸ªè¿›ç¨‹æœ‰ç‹¬ç«‹çš„å†…å­˜ç©ºé—´ï¼Œé¿å…æ•°æ®ç«äº‰
4. **å®¹é”™æ€§**ï¼šå•ä¸ªè¿›ç¨‹å´©æºƒä¸ä¼šå½±å“å…¶ä»–è¿›ç¨‹

**æŠ€æœ¯ç»†èŠ‚ï¼š**
```python
# å¤šçº¿ç¨‹å—GILé™åˆ¶
import threading
def cpu_intensive_task():
    for i in range(1000000):
        math.sqrt(i)

# å¤šè¿›ç¨‹ç»•è¿‡GIL
import multiprocessing
def cpu_intensive_task():
    for i in range(1000000):
        math.sqrt(i)
```

### Q2: å¦‚ä½•å¤„ç†è¿›ç¨‹é—´é€šä¿¡ï¼Ÿ

**å›ç­”è¦ç‚¹ï¼š**
1. **æ•°æ®åºåˆ—åŒ–**ï¼šä½¿ç”¨pickleåºåˆ—åŒ–å¤æ‚å¯¹è±¡
2. **å…±äº«å†…å­˜**ï¼šå¯¹äºå¤§å‹æ•°ç»„ä½¿ç”¨multiprocessing.shared_memory
3. **é˜Ÿåˆ—é€šä¿¡**ï¼šä½¿ç”¨Queueè¿›è¡Œè¿›ç¨‹é—´æ¶ˆæ¯ä¼ é€’
4. **æ–‡ä»¶ç³»ç»Ÿ**ï¼šé€šè¿‡ä¸´æ—¶æ–‡ä»¶ä¼ é€’ç»“æœ

**ä»£ç ç¤ºä¾‹ï¼š**
```python
# å…±äº«å†…å­˜ç¤ºä¾‹
import multiprocessing as mp
from multiprocessing import shared_memory

def create_shared_array(data):
    shm = shared_memory.SharedMemory(create=True, size=data.nbytes)
    shared_array = np.ndarray(data.shape, dtype=data.dtype, buffer=shm.buf)
    shared_array[:] = data[:]
    return shm.name, data.shape, data.dtype

def access_shared_array(shm_name, shape, dtype):
    shm = shared_memory.SharedMemory(name=shm_name)
    return np.ndarray(shape, dtype=dtype, buffer=shm.buf)
```

### Q3: å¦‚ä½•ä¿è¯æ•°æ®ä¸€è‡´æ€§ï¼Ÿ

**å›ç­”è¦ç‚¹ï¼š**
1. **åŸå­æ€§æ“ä½œ**ï¼šä½¿ç”¨æ–‡ä»¶é”ç¡®ä¿å†™å…¥æ“ä½œçš„åŸå­æ€§
2. **ç»“æœéªŒè¯**ï¼šå¯¹æ¯ä¸ªç»“æœè¿›è¡Œå®Œæ•´æ€§æ£€æŸ¥
3. **é”™è¯¯å¤„ç†**ï¼šå®ç°é‡è¯•æœºåˆ¶å’Œé”™è¯¯æ¢å¤
4. **çŠ¶æ€ç®¡ç†**ï¼šç»´æŠ¤å¤„ç†çŠ¶æ€ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 

**å®ç°æ–¹æ¡ˆï¼š**
```python
import fcntl
import json
from pathlib import Path

class AtomicWriter:
    def write_result(self, filepath, data):
        temp_file = Path(f"{filepath}.tmp")
        
        # å†™å…¥ä¸´æ—¶æ–‡ä»¶
        with open(temp_file, 'w') as f:
            json.dump(data, f)
        
        # åŸå­æ€§é‡å‘½å
        temp_file.rename(filepath)
```

### Q4: å¦‚ä½•ç›‘æ§å¤šè¿›ç¨‹çš„æ‰§è¡ŒçŠ¶æ€ï¼Ÿ

**å›ç­”è¦ç‚¹ï¼š**
1. **è¿›åº¦è·Ÿè¸ª**ï¼šä½¿ç”¨tqdmæ˜¾ç¤ºå®æ—¶è¿›åº¦
2. **æ—¥å¿—ç®¡ç†**ï¼šæ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹æ—¥å¿—ï¼Œä¸»è¿›ç¨‹æ±‡æ€»
3. **æ€§èƒ½ç›‘æ§**ï¼šç›‘æ§CPUã€å†…å­˜ä½¿ç”¨æƒ…å†µ
4. **é”™è¯¯æ”¶é›†**ï¼šæ”¶é›†å’Œåˆ†ç±»å„ç§é”™è¯¯

**ç›‘æ§å®ç°ï¼š**
```python
from tqdm import tqdm
import psutil
import logging

class ProcessMonitor:
    def __init__(self, total_tasks):
        self.pbar = tqdm(total=total_tasks)
        self.logger = logging.getLogger('batch_processor')
    
    def update_progress(self, completed=1):
        self.pbar.update(completed)
    
    def log_system_stats(self):
        cpu_percent = psutil.cpu_percent()
        memory_percent = psutil.virtual_memory().percent
        self.logger.info(f"CPU: {cpu_percent}%, Memory: {memory_percent}%")
```

### Q5: å¦‚ä½•å¤„ç†å†…å­˜æ³„æ¼é—®é¢˜ï¼Ÿ

**å›ç­”è¦ç‚¹ï¼š**
1. **èµ„æºç®¡ç†**ï¼šä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿èµ„æºé‡Šæ”¾
2. **å†…å­˜ç›‘æ§**ï¼šå®šæœŸæ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
3. **åƒåœ¾å›æ”¶**ï¼šä¸»åŠ¨è§¦å‘åƒåœ¾å›æ”¶
4. **è¿›ç¨‹é‡å¯**ï¼šå®šæœŸé‡å¯å·¥ä½œè¿›ç¨‹

**å†…å­˜ç®¡ç†ï¼š**
```python
import gc
import psutil
import os

class MemoryManager:
    def __init__(self, max_memory_mb=1000):
        self.max_memory = max_memory_mb * 1024 * 1024
    
    def check_memory(self):
        process = psutil.Process(os.getpid())
        memory_mb = process.memory_info().rss / 1024 / 1024
        return memory_mb
    
    def cleanup_if_needed(self):
        if self.check_memory() > self.max_memory / 1024 / 1024:
            gc.collect()
            return True
        return False
```

### Q6: å¦‚ä½•ä¼˜åŒ–å¤šè¿›ç¨‹çš„æ€§èƒ½ï¼Ÿ

**å›ç­”è¦ç‚¹ï¼š**
1. **è¿›ç¨‹æ•°ä¼˜åŒ–**ï¼šæ ¹æ®CPUæ ¸å¿ƒæ•°å’Œä»»åŠ¡ç‰¹æ€§ç¡®å®šæœ€ä¼˜è¿›ç¨‹æ•°
2. **æ‰¹å¤„ç†å¤§å°**ï¼šè°ƒæ•´æ¯ä¸ªè¿›ç¨‹å¤„ç†çš„ä»»åŠ¡æ•°é‡
3. **å†…å­˜ä¼˜åŒ–**ï¼šä½¿ç”¨å…±äº«å†…å­˜å‡å°‘æ•°æ®å¤åˆ¶
4. **I/Oä¼˜åŒ–**ï¼šå¼‚æ­¥I/Oå‡å°‘ç­‰å¾…æ—¶é—´

**æ€§èƒ½ä¼˜åŒ–ç­–ç•¥ï¼š**
```python
def optimize_process_count(cpu_count, sample_count, avg_time_per_sample):
    """è®¡ç®—æœ€ä¼˜è¿›ç¨‹æ•°"""
    # è€ƒè™‘è¿›ç¨‹åˆ›å»ºå¼€é”€
    process_overhead = 0.1  # 10%å¼€é”€
    
    # è®¡ç®—ç†è®ºæœ€ä¼˜è¿›ç¨‹æ•°
    optimal_processes = min(
        cpu_count,
        int(sample_count * avg_time_per_sample / (1 + process_overhead))
    )
    
    return max(1, optimal_processes)
```

---

## ä»£ç ç¤ºä¾‹

### å®Œæ•´çš„å¤šè¿›ç¨‹æ‰¹å¤„ç†å®ç°

```python
"""
é‡å­å±‚æå¤šè¿›ç¨‹æ‰¹å¤„ç†å®ç°
æ”¯æŒçº¿æ€§é‡æ„å’ŒMLEé‡æ„çš„å¹¶è¡Œå¤„ç†
"""

import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
from pathlib import Path
import pickle
import tempfile
import shutil
import json
import time
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass
import numpy as np
import pandas as pd
from tqdm import tqdm

@dataclass
class MultiProcessConfig:
    """å¤šè¿›ç¨‹é…ç½®"""
    max_workers: int = None
    chunk_size: int = 1
    timeout: int = 600  # 10åˆ†é’Ÿè¶…æ—¶
    memory_limit_mb: int = 1000
    enable_progress: bool = True
    retry_failed: bool = True
    max_retries: int = 3

class MultiProcessBatchController:
    """å¤šè¿›ç¨‹æ‰¹å¤„ç†æ§åˆ¶å™¨"""
    
    def __init__(self, config: MultiProcessConfig = None):
        self.config = config or MultiProcessConfig()
        self.temp_dir = None
        self.shared_objects = {}
        self.progress_tracker = None
    
    def run_batch(self, reconstruction_config, data: np.ndarray) -> Dict[str, Any]:
        """æ‰§è¡Œå¤šè¿›ç¨‹æ‰¹å¤„ç†"""
        print(f"å¼€å§‹å¤šè¿›ç¨‹æ‰¹å¤„ç†ï¼Œæ ·æœ¬æ•°: {data.shape[1]}")
        
        # 1. å‡†å¤‡é˜¶æ®µ
        self._prepare_environment(reconstruction_config, data)
        
        try:
            # 2. å¤šè¿›ç¨‹å¤„ç†
            results = self._process_samples_parallel(data, reconstruction_config)
            
            # 3. æ”¶é›†ç»“æœ
            return self._collect_results(results, reconstruction_config)
            
        finally:
            # 4. æ¸…ç†
            self._cleanup()
    
    def _prepare_environment(self, config, data):
        """å‡†å¤‡å¤„ç†ç¯å¢ƒ"""
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        self.temp_dir = Path(tempfile.mkdtemp(prefix="qtomography_batch_"))
        
        # å‡†å¤‡å…±äº«å¯¹è±¡
        self._prepare_shared_objects(config, data.shape[0])
        
        # åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ª
        if self.config.enable_progress:
            self.progress_tracker = tqdm(
                total=data.shape[1], 
                desc="æ‰¹å¤„ç†è¿›åº¦",
                unit="æ ·æœ¬"
            )
    
    def _prepare_shared_objects(self, config, num_samples):
        """å‡†å¤‡å…±äº«å¯¹è±¡"""
        # åºåˆ—åŒ–é…ç½®
        self.shared_objects['config'] = {
            'dimension': config.dimension,
            'methods': config.methods,
            'tolerance': config.tolerance,
            'linear_regularization': config.linear_regularization,
            'mle_regularization': config.mle_regularization,
            'mle_max_iterations': config.mle_max_iterations,
        }
        
        # é¢„è®¡ç®—æŠ•å½±ç®—å­ï¼ˆå¦‚æœå¯ç”¨ç¼“å­˜ï¼‰
        if config.cache_projectors:
            # è¿™é‡Œåº”è¯¥é¢„è®¡ç®—æŠ•å½±ç®—å­
            # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬å‡è®¾åœ¨å­è¿›ç¨‹ä¸­è®¡ç®—
            pass
    
    def _process_samples_parallel(self, data, config):
        """å¹¶è¡Œå¤„ç†æ ·æœ¬"""
        sample_args = self._prepare_sample_args(data, config)
        
        results = []
        max_workers = self.config.max_workers or min(mp.cpu_count(), 8)
        
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_idx = {
                executor.submit(
                    process_single_sample_worker, 
                    args
                ): idx for idx, args in enumerate(sample_args)
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_idx, timeout=self.config.timeout):
                idx = future_to_idx[future]
                try:
                    result = future.result()
                    results.append((idx, result))
                    
                    if self.progress_tracker:
                        self.progress_tracker.update(1)
                        
                except Exception as e:
                    print(f"æ ·æœ¬ {idx} å¤„ç†å¤±è´¥: {e}")
                    results.append((idx, None))
                    
                    if self.progress_tracker:
                        self.progress_tracker.update(1)
        
        return results
    
    def _prepare_sample_args(self, data, config):
        """å‡†å¤‡æ ·æœ¬å¤„ç†å‚æ•°"""
        args = []
        for idx in range(data.shape[1]):
            arg = (
                idx,                                    # æ ·æœ¬ç´¢å¼•
                data[:, idx].tolist(),                 # æ¦‚ç‡å‘é‡ï¼ˆè½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¾¿åºåˆ—åŒ–ï¼‰
                self.shared_objects['config'],         # é…ç½®å­—å…¸
                str(self.temp_dir)                     # ä¸´æ—¶ç›®å½•è·¯å¾„
            )
            args.append(arg)
        return args
    
    def _collect_results(self, results, config):
        """æ”¶é›†å’Œæ±‡æ€»ç»“æœ"""
        successful_results = []
        failed_samples = []
        
        for idx, result in results:
            if result is None:
                failed_samples.append(idx)
                continue
                
            # ä»ä¸´æ—¶æ–‡ä»¶åŠ è½½ç»“æœ
            result_file = Path(result)
            if result_file.exists():
                try:
                    with open(result_file, 'rb') as f:
                        sample_result = pickle.load(f)
                    successful_results.append((idx, sample_result))
                except Exception as e:
                    print(f"åŠ è½½æ ·æœ¬ {idx} ç»“æœå¤±è´¥: {e}")
                    failed_samples.append(idx)
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        summary = {
            'total_samples': len(results),
            'successful_samples': len(successful_results),
            'failed_samples': len(failed_samples),
            'success_rate': len(successful_results) / len(results) if results else 0,
            'results': successful_results,
            'failed_indices': failed_samples
        }
        
        return summary
    
    def _cleanup(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        if self.temp_dir and self.temp_dir.exists():
            shutil.rmtree(self.temp_dir, ignore_errors=True)
        
        if self.progress_tracker:
            self.progress_tracker.close()

def process_single_sample_worker(args):
    """å·¥ä½œè¿›ç¨‹å‡½æ•°ï¼ˆå¿…é¡»åœ¨æ¨¡å—çº§åˆ«å®šä¹‰ï¼‰"""
    idx, probs, config_dict, temp_dir = args
    
    try:
        # é‡å»ºé‡æ„å™¨ï¼ˆåœ¨å­è¿›ç¨‹ä¸­ï¼‰
        from qtomography.domain.reconstruction.linear import LinearReconstructor
        from qtomography.domain.reconstruction.mle import MLEReconstructor
        
        dimension = config_dict['dimension']
        methods = config_dict['methods']
        
        results = {}
        
        # çº¿æ€§é‡æ„
        if 'linear' in methods:
            linear = LinearReconstructor(
                dimension=dimension,
                tolerance=config_dict['tolerance'],
                regularization=config_dict['linear_regularization'],
                cache_projectors=False  # å­è¿›ç¨‹ä¸­ä¸ç¼“å­˜
            )
            linear_result = linear.reconstruct_with_details(np.array(probs))
            results['linear'] = _serialize_reconstruction_result(linear_result)
        
        # MLEé‡æ„
        if 'mle' in methods:
            mle = MLEReconstructor(
                dimension=dimension,
                tolerance=config_dict['tolerance'],
                regularization=config_dict['mle_regularization'],
                max_iterations=config_dict['mle_max_iterations'],
                cache_projectors=False
            )
            
            # ä½¿ç”¨çº¿æ€§ç»“æœä½œä¸ºåˆå§‹å€¼
            initial_density = None
            if 'linear' in results:
                initial_density = results['linear']['density_matrix']
            
            mle_result = mle.reconstruct_with_details(
                np.array(probs), 
                initial_density=initial_density
            )
            results['mle'] = _serialize_reconstruction_result(mle_result)
        
        # ä¿å­˜ç»“æœåˆ°ä¸´æ—¶æ–‡ä»¶
        result_file = Path(temp_dir) / f"sample_{idx}.pkl"
        with open(result_file, 'wb') as f:
            pickle.dump(results, f)
        
        return str(result_file)
        
    except Exception as e:
        # ä¿å­˜é”™è¯¯ä¿¡æ¯
        error_file = Path(temp_dir) / f"error_{idx}.pkl"
        error_info = {
            'error': str(e),
            'error_type': type(e).__name__,
            'sample_index': idx
        }
        with open(error_file, 'wb') as f:
            pickle.dump(error_info, f)
        return str(error_file)

def _serialize_reconstruction_result(result):
    """åºåˆ—åŒ–é‡æ„ç»“æœ"""
    return {
        'density_matrix': result.density.matrix.tolist(),
        'purity': float(result.density.purity),
        'trace': float(np.real(result.density.trace)),
        'eigenvalues': result.density.eigenvalues.tolist(),
        'normalized_probabilities': result.normalized_probabilities.tolist(),
        # çº¿æ€§é‡æ„ç‰¹æœ‰
        'residuals': result.residuals.tolist() if hasattr(result, 'residuals') else None,
        'rank': result.rank if hasattr(result, 'rank') else None,
        'singular_values': result.singular_values.tolist() if hasattr(result, 'singular_values') else None,
        # MLEé‡æ„ç‰¹æœ‰
        'objective_value': result.objective_value if hasattr(result, 'objective_value') else None,
        'n_iterations': result.n_iterations if hasattr(result, 'n_iterations') else None,
        'n_function_evaluations': result.n_function_evaluations if hasattr(result, 'n_function_evaluations') else None,
        'success': result.success if hasattr(result, 'success') else None,
        'status': result.status if hasattr(result, 'status') else None,
    }

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # é…ç½®å¤šè¿›ç¨‹æ‰¹å¤„ç†
    mp_config = MultiProcessConfig(
        max_workers=4,
        timeout=300,
        enable_progress=True
    )
    
    # åˆ›å»ºæ§åˆ¶å™¨
    controller = MultiProcessBatchController(mp_config)
    
    # å‡†å¤‡æ•°æ®ï¼ˆç¤ºä¾‹ï¼‰
    data = np.random.rand(16, 100)  # 16ä¸ªæµ‹é‡ï¼Œ100ä¸ªæ ·æœ¬
    data = data / data.sum(axis=0)  # å½’ä¸€åŒ–
    
    # é‡æ„é…ç½®
    from qtomography.app.controller import ReconstructionConfig
    config = ReconstructionConfig(
        input_path="dummy.csv",
        output_dir="output/",
        methods=["linear", "mle"],
        dimension=4,
        cache_projectors=True
    )
    
    # æ‰§è¡Œæ‰¹å¤„ç†
    results = controller.run_batch(config, data)
    
    print(f"æ‰¹å¤„ç†å®Œæˆ:")
    print(f"æ€»æ ·æœ¬æ•°: {results['total_samples']}")
    print(f"æˆåŠŸæ ·æœ¬æ•°: {results['successful_samples']}")
    print(f"æˆåŠŸç‡: {results['success_rate']:.2%}")
```

---

## æ€»ç»“

### å…³é”®æŠ€æœ¯ç‚¹
1. **è¿›ç¨‹é—´é€šä¿¡**ï¼šä½¿ç”¨åºåˆ—åŒ–å’Œå…±äº«å†…å­˜
2. **å†…å­˜ç®¡ç†**ï¼šä¼˜åŒ–æ•°æ®ä¼ é€’å’Œå­˜å‚¨
3. **é”™è¯¯å¤„ç†**ï¼šå®ç°å®¹é”™å’Œé‡è¯•æœºåˆ¶
4. **æ€§èƒ½ç›‘æ§**ï¼šå®æ—¶è¿›åº¦å’Œèµ„æºç›‘æ§
5. **æ•°æ®ä¸€è‡´æ€§**ï¼šåŸå­æ€§æ“ä½œå’ŒçŠ¶æ€ç®¡ç†

### é¢è¯•ä¼˜åŠ¿
- å±•ç¤ºäº†æ·±å…¥çš„ç³»ç»Ÿè®¾è®¡èƒ½åŠ›
- ä½“ç°äº†å¯¹å¹¶å‘ç¼–ç¨‹çš„ç†è§£
- è¯æ˜äº†è§£å†³å¤æ‚æŠ€æœ¯é—®é¢˜çš„èƒ½åŠ›
- æ˜¾ç¤ºäº†æ€§èƒ½ä¼˜åŒ–çš„å®è·µç»éªŒ

### æ³¨æ„äº‹é¡¹
- å¼ºè°ƒè¿™æ˜¯é’ˆå¯¹ç‰¹å®šåœºæ™¯çš„ä¼˜åŒ–
- è¯´æ˜å•è¿›ç¨‹ç‰ˆæœ¬å·²ç»è¶³å¤Ÿå¿«
- å±•ç¤ºäº†å¯¹æŠ€æœ¯é€‰å‹çš„æ€è€ƒè¿‡ç¨‹
- ä½“ç°äº†å·¥ç¨‹å®è·µä¸­çš„æƒè¡¡è€ƒè™‘

è¿™ä¸ªæ–‡æ¡£æ¶µç›–äº†å¤šè¿›ç¨‹æ‰¹å¤„ç†çš„æ‰€æœ‰å…³é”®æŠ€æœ¯ç‚¹ï¼Œå¯ä»¥ä½œä¸ºé¢è¯•æ—¶çš„æŠ€æœ¯å‚è€ƒå’Œè®¨è®ºåŸºç¡€ã€‚
